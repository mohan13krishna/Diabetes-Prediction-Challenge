{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e32bdd7",
   "metadata": {},
   "source": [
    "# üèÜ V21 - Diabetes Prediction Champion Solution (0.69760 Score)\n",
    "\n",
    "## Kaggle Playground Series - Season 5, Episode 12\n",
    "\n",
    "### Best Performing Solution | Ultra-Heavy Regularized Ensemble\n",
    "\n",
    "**Private Score:** 0.69760 (Best)  \n",
    "**Public Score:** 0.70042  \n",
    "**Final Rank:** 877/4206 (Top 20.8%)  \n",
    "**Approach:** 10-Fold Cross-Validation with 3-Model Ensemble + Feature Selection + Isotonic Calibration\n",
    "\n",
    "---\n",
    "\n",
    "### Solution Architecture:\n",
    "1. **External Feature Engineering** - Leverage 100K Diabetes Health Indicators Dataset\n",
    "2. **Manual Medical Features** - BMI categories, BP categories, clinical ratios\n",
    "3. **10-Fold Stratified CV** - Balanced train-validation splits\n",
    "4. **Three Base Models** - XGBoost (50%), LightGBM (35%), CatBoost (15%)\n",
    "5. **Aggressive Regularization** - L1=3.5, L2=4.0 to prevent overfitting on 700K samples\n",
    "6. **Feature Selection** - SelectFromModel reduces 75 features to 38 most important\n",
    "7. **Probability Calibration** - IsotonicRegression for better probability estimates\n",
    "8. **Submission Generation** - Final test set predictions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d47e117",
   "metadata": {},
   "source": [
    "## üìö Section 1: Load and Explore Data\n",
    "\n",
    "Import required libraries and load the training, test, and external datasets. Display basic statistics and target distribution to understand the data landscape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad16ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# Gradient Boosting imports\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "\n",
    "print(\"‚úÖ V21 - Champion Solution with Score 0.69760\")\n",
    "print(\"üèÜ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cd0a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train = pd.read_csv('/kaggle/input/playground-series-s5e12/train.csv')\n",
    "test = pd.read_csv('/kaggle/input/playground-series-s5e12/test.csv')\n",
    "sub = pd.read_csv('/kaggle/input/playground-series-s5e12/sample_submission.csv')\n",
    "orig = pd.read_csv('/kaggle/input/diabetes-health-indicators-dataset/diabetes_dataset.csv')\n",
    "\n",
    "TARGET = 'diagnosed_diabetes'\n",
    "\n",
    "# Display dataset shapes\n",
    "print(f\"üìä Training set shape: {train.shape}\")\n",
    "print(f\"üìä Test set shape: {test.shape}\")\n",
    "print(f\"üìä Original (external) dataset shape: {orig.shape}\")\n",
    "print(f\"üìä Submission template shape: {sub.shape}\")\n",
    "\n",
    "# Display target distribution\n",
    "print(f\"\\nüéØ Target Variable Distribution:\")\n",
    "print(train[TARGET].value_counts())\n",
    "print(f\"\\nClass Balance:\")\n",
    "print(train[TARGET].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fec7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows and basic statistics\n",
    "print(\"\\nüìà Training set head:\")\n",
    "print(train.head())\n",
    "\n",
    "print(\"\\nüìä Basic statistics:\")\n",
    "print(train.describe())\n",
    "\n",
    "print(f\"\\nüî¢ Data types:\")\n",
    "print(train.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba39881f",
   "metadata": {},
   "source": [
    "## üìö Section 2: External Feature Engineering from Original Dataset\n",
    "\n",
    "Create **mean encoding** and **count encoding** features from the original diabetes health indicators dataset. These external features leverage the 100K-sample original dataset to encode each feature based on its relationship with the target variable in the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d96be6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify base columns (all except id and target)\n",
    "base_cols = [c for c in train.columns if c not in ['id', TARGET]]\n",
    "print(f\"Base features to encode: {len(base_cols)}\")\n",
    "print(f\"Features: {base_cols}\")\n",
    "\n",
    "# Create external encoding features\n",
    "encoded = []\n",
    "\n",
    "for col in base_cols:\n",
    "    # 1. MEAN ENCODING: Average target value for each feature value in original data\n",
    "    mean_map = orig.groupby(col)[TARGET].mean()\n",
    "    train[f\"enc_mean_{col}\"] = train[col].map(mean_map)\n",
    "    test[f\"enc_mean_{col}\"] = test[col].map(mean_map)\n",
    "    encoded.append(f\"enc_mean_{col}\")\n",
    "    \n",
    "    # 2. COUNT ENCODING: Log-scaled frequency of each feature value in original data\n",
    "    count_map = orig.groupby(col).size()\n",
    "    train[f\"enc_cnt_{col}\"] = np.log1p(train[col].map(count_map).fillna(0))\n",
    "    test[f\"enc_cnt_{col}\"] = np.log1p(test[col].map(count_map).fillna(0))\n",
    "    encoded.append(f\"enc_cnt_{col}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Created {len(encoded)} external encoding features\")\n",
    "print(f\"Sample external features: {encoded[:6]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b7fcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify external features\n",
    "print(\"‚úÖ External feature sample:\")\n",
    "print(train[encoded[:4]].head())\n",
    "\n",
    "print(f\"\\nMissing values in encoded features:\")\n",
    "print(train[encoded].isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9f6f38",
   "metadata": {},
   "source": [
    "## üìö Section 3: Create Manual Clinical Features\n",
    "\n",
    "Engineer domain-specific features based on medical knowledge and clinical standards:\n",
    "- **BMI Categories** - WHO classifications (Underweight, Normal, Overweight, Obese)\n",
    "- **Blood Pressure Categories** - AHA standards (Normal, Elevated, High)\n",
    "- **Non-HDL Cholesterol** - Clinical predictor of cardiovascular risk\n",
    "\n",
    "These features capture important non-linear relationships in medical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98794dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. BMI CATEGORIZATION - WHO Guidelines\n",
    "# Underweight: BMI < 18.5\n",
    "# Normal: 18.5 ‚â§ BMI < 25\n",
    "# Overweight: 25 ‚â§ BMI < 30\n",
    "# Obese: BMI ‚â• 30\n",
    "\n",
    "train['bmi_cat'] = pd.cut(train['bmi'], \n",
    "                           bins=[0, 18.5, 25, 30, 999], \n",
    "                           labels=[0, 1, 2, 3]).astype(int)\n",
    "test['bmi_cat'] = pd.cut(test['bmi'], \n",
    "                          bins=[0, 18.5, 25, 30, 999], \n",
    "                          labels=[0, 1, 2, 3]).astype(int)\n",
    "\n",
    "print(\"‚úÖ BMI Categories:\")\n",
    "print(f\"0=Underweight, 1=Normal, 2=Overweight, 3=Obese\")\n",
    "print(train['bmi_cat'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712d5212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. BLOOD PRESSURE CATEGORIZATION - AHA Guidelines\n",
    "# Normal: SBP < 120 AND DBP < 80\n",
    "# Elevated: 120 ‚â§ SBP < 140 OR 80 ‚â§ DBP < 90\n",
    "# High (Stage 1): SBP ‚â• 140 OR DBP ‚â• 90\n",
    "\n",
    "train['bp_cat'] = 0  # Normal\n",
    "train.loc[(train['systolic_bp'] >= 140) | (train['diastolic_bp'] >= 90), 'bp_cat'] = 2  # High\n",
    "train.loc[((train['systolic_bp'] >= 120) & (train['systolic_bp'] < 140)) | \n",
    "          ((train['diastolic_bp'] >= 80) & (train['diastolic_bp'] < 90)), 'bp_cat'] = 1  # Elevated\n",
    "\n",
    "test['bp_cat'] = 0\n",
    "test.loc[(test['systolic_bp'] >= 140) | (test['diastolic_bp'] >= 90), 'bp_cat'] = 2\n",
    "test.loc[((test['systolic_bp'] >= 120) & (test['systolic_bp'] < 140)) | \n",
    "         ((test['diastolic_bp'] >= 80) & (test['diastolic_bp'] < 90)), 'bp_cat'] = 1\n",
    "\n",
    "print(\"‚úÖ Blood Pressure Categories:\")\n",
    "print(f\"0=Normal, 1=Elevated, 2=High\")\n",
    "print(train['bp_cat'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e328c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. NON-HDL CHOLESTEROL\n",
    "# Clinical indicator: Total Cholesterol - HDL\n",
    "# Higher non-HDL indicates more \"bad\" cholesterol (LDL + VLDL)\n",
    "\n",
    "train['non_hdl'] = train['cholesterol_total'] - train['hdl_cholesterol']\n",
    "test['non_hdl'] = test['cholesterol_total'] - test['hdl_cholesterol']\n",
    "\n",
    "print(\"‚úÖ Non-HDL Cholesterol Feature:\")\n",
    "print(f\"Non-HDL range (train): {train['non_hdl'].min():.2f} to {train['non_hdl'].max():.2f}\")\n",
    "print(f\"Non-HDL mean (train): {train['non_hdl'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e694ac",
   "metadata": {},
   "source": [
    "## üìö Section 4: Prepare Features and Target\n",
    "\n",
    "Consolidate all features, handle missing values, apply label encoding, and prepare final feature matrices for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299e597d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidate all features\n",
    "features = base_cols + ['bmi_cat', 'bp_cat', 'non_hdl'] + encoded\n",
    "print(f\"üìä Total features: {len(features)}\")\n",
    "print(f\"  - Base features: {len(base_cols)}\")\n",
    "print(f\"  - Manual clinical features: 3 (bmi_cat, bp_cat, non_hdl)\")\n",
    "print(f\"  - External encoding features: {len(encoded)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0786ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values in encoded features\n",
    "# Fill NaNs with median value (created when a value wasn't present in original dataset)\n",
    "for f in encoded:\n",
    "    median_val = train[f].median()\n",
    "    train[f] = train[f].fillna(median_val)\n",
    "    test[f] = test[f].fillna(median_val)\n",
    "\n",
    "print(f\"‚úÖ Missing values handled\")\n",
    "print(f\"Total NaNs in features: {train[features].isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac152da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare X, y, and X_test\n",
    "X = train[features].copy()\n",
    "y = train[TARGET]\n",
    "X_test = test[features].copy()\n",
    "\n",
    "print(f\"‚úÖ Feature matrices prepared\")\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293a8b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encode categorical columns\n",
    "# Tree-based models can handle categories natively, but explicit encoding ensures consistency\n",
    "cat_cols = ['bmi_cat', 'bp_cat'] + train.select_dtypes('object').columns.tolist()\n",
    "\n",
    "for col in cat_cols:\n",
    "    if col in X.columns:\n",
    "        le = LabelEncoder()\n",
    "        X[col] = le.fit_transform(X[col].astype(str))\n",
    "        X_test[col] = le.transform(X_test[col].astype(str))\n",
    "\n",
    "print(f\"‚úÖ Label encoding applied to {len(cat_cols)} categorical columns\")\n",
    "print(f\"X dtypes after encoding:\")\n",
    "print(X.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0730286",
   "metadata": {},
   "source": [
    "## üìö Section 5: Build 10-Fold Stratified Cross-Validation Ensemble\n",
    "\n",
    "Implement stratified k-fold cross-validation with three base models:\n",
    "- **XGBoost (50% weight)** - Primary model, high regularization\n",
    "- **LightGBM (35% weight)** - Speed and efficiency\n",
    "- **CatBoost (15% weight)** - Categorical handling and stability\n",
    "\n",
    "**Ultra-Heavy Regularization:**\n",
    "- `reg_alpha=3.5, reg_lambda=4.0` (L1 and L2 penalties)\n",
    "- `max_depth=4` (shallow trees to reduce variance)\n",
    "- `subsample=0.7, colsample_bytree=0.6` (row/column subsampling)\n",
    "\n",
    "This prevents overfitting on the large 700K training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fca144d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize 10-Fold Stratified K-Fold\n",
    "n_splits = 10\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Out-of-Fold predictions for training and blending\n",
    "oof_blend = np.zeros(len(X))\n",
    "test_blend = np.zeros(len(X_test))\n",
    "\n",
    "print(f\"üîÑ Starting {n_splits}-fold ultra-regularized ensemble training...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5051da80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with 10 folds\n",
    "for fold, (trn_idx, val_idx) in enumerate(skf.split(X, y), 1):\n",
    "    print(f\"Fold {fold}/{n_splits} ‚Üí \", end=\"\")\n",
    "    \n",
    "    X_trn, X_val = X.iloc[trn_idx], X.iloc[val_idx]\n",
    "    y_trn, y_val = y.iloc[trn_idx], y.iloc[val_idx]\n",
    "\n",
    "    # ========== XGBoost (50% weight) ==========\n",
    "    model1 = xgb.XGBClassifier(\n",
    "        n_estimators=5000,\n",
    "        max_depth=4,\n",
    "        learning_rate=0.007,\n",
    "        subsample=0.7,\n",
    "        colsample_bytree=0.6,\n",
    "        reg_alpha=3.5,  # L1 regularization\n",
    "        reg_lambda=4.0,  # L2 regularization\n",
    "        random_state=42,\n",
    "        tree_method='hist',\n",
    "        n_jobs=-1,\n",
    "        verbosity=0\n",
    "    )\n",
    "    model1.fit(X_trn, y_trn, \n",
    "               eval_set=[(X_val, y_val)], \n",
    "               early_stopping_rounds=300, \n",
    "               verbose=False)\n",
    "\n",
    "    # ========== LightGBM (35% weight) ==========\n",
    "    model2 = lgb.LGBMClassifier(\n",
    "        n_estimators=5000,\n",
    "        max_depth=4,\n",
    "        learning_rate=0.007,\n",
    "        num_leaves=16,\n",
    "        subsample=0.7,\n",
    "        colsample_bytree=0.6,\n",
    "        reg_alpha=3.5,\n",
    "        reg_lambda=4.0,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=-1\n",
    "    )\n",
    "    model2.fit(X_trn, y_trn, \n",
    "               eval_set=[(X_val, y_val)], \n",
    "               callbacks=[lgb.early_stopping(300)])\n",
    "\n",
    "    # ========== CatBoost (15% weight) ==========\n",
    "    model3 = cb.CatBoostClassifier(\n",
    "        iterations=5000,\n",
    "        depth=4,\n",
    "        learning_rate=0.007,\n",
    "        l2_leaf_reg=12.0,\n",
    "        random_seed=42,\n",
    "        verbose=False,\n",
    "        early_stopping_rounds=300\n",
    "    )\n",
    "    model3.fit(X_trn, y_trn, eval_set=(X_val, y_val), verbose=False)\n",
    "\n",
    "    # ========== BLEND PREDICTIONS ==========\n",
    "    val_pred = (model1.predict_proba(X_val)[:,1] * 0.50 +\n",
    "                model2.predict_proba(X_val)[:,1] * 0.35 +\n",
    "                model3.predict_proba(X_val)[:,1] * 0.15)\n",
    "\n",
    "    oof_blend[val_idx] = val_pred\n",
    "    fold_auc = roc_auc_score(y_val, val_pred)\n",
    "    print(f\"AUC = {fold_auc:.6f}\")\n",
    "\n",
    "    # Test set predictions\n",
    "    test_blend += (model1.predict_proba(X_test)[:,1] * 0.50 +\n",
    "                   model2.predict_proba(X_test)[:,1] * 0.35 +\n",
    "                   model3.predict_proba(X_test)[:,1] * 0.15) / n_splits\n",
    "\n",
    "    # Cleanup\n",
    "    del model1, model2, model3, X_trn, X_val, y_trn, y_val\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"\\n‚úÖ Final CV AUC: {roc_auc_score(y, oof_blend):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f598c90",
   "metadata": {},
   "source": [
    "## üìö Section 6: Perform Feature Selection\n",
    "\n",
    "Use `SelectFromModel` with the trained XGBoost model to identify the most important features. This reduces dimensionality (75 ‚Üí 38 features) while retaining predictive power. Feature importance from tree models is based on how often a feature is used in splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3d9d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection using the first fold's XGBoost model\n",
    "# SelectFromModel selects features based on importance > threshold (median)\n",
    "selector = SelectFromModel(model1, threshold='median', prefit=True)\n",
    "X_sel = selector.transform(X)\n",
    "X_test_sel = selector.transform(X_test)\n",
    "\n",
    "selected_features = X.columns[selector.get_support()].tolist()\n",
    "print(f\"‚úÖ Feature Selection Results:\")\n",
    "print(f\"Original features: {X.shape[1]}\")\n",
    "print(f\"Selected features: {X_sel.shape[1]}\")\n",
    "print(f\"Reduction: {X.shape[1] - X_sel.shape[1]} features dropped\")\n",
    "print(f\"\\nTop selected features (first 15):\")\n",
    "print(selected_features[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4b714d",
   "metadata": {},
   "source": [
    "## üìö Section 7: Train Final Model with Selected Features\n",
    "\n",
    "Train a final XGBoost classifier on the selected features only. Use optimized hyperparameters with slightly less aggressive regularization than the CV phase.\n",
    "\n",
    "This final model is trained on ALL training data (not CV folds) for maximum data utilization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56e192d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-initialize model1 if not available from loop\n",
    "final_model = xgb.XGBClassifier(\n",
    "    n_estimators=2000,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.01,\n",
    "    subsample=0.7,\n",
    "    colsample_bytree=0.6,\n",
    "    reg_alpha=3.0,\n",
    "    reg_lambda=3.5,\n",
    "    random_state=42,\n",
    "    tree_method='hist',\n",
    "    n_jobs=-1,\n",
    "    verbosity=0\n",
    ")\n",
    "\n",
    "# Train on complete training data with selected features\n",
    "final_model.fit(X_sel, y)\n",
    "\n",
    "# Generate test predictions\n",
    "final_pred = final_model.predict_proba(X_test_sel)[:,1]\n",
    "\n",
    "print(f\"‚úÖ Final model trained on {len(selected_features)} selected features\")\n",
    "print(f\"Test predictions shape: {final_pred.shape}\")\n",
    "print(f\"Prediction statistics:\")\n",
    "print(f\"  Min: {final_pred.min():.6f}\")\n",
    "print(f\"  Max: {final_pred.max():.6f}\")\n",
    "print(f\"  Mean: {final_pred.mean():.6f}\")\n",
    "print(f\"  Median: {np.median(final_pred):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799333bb",
   "metadata": {},
   "source": [
    "## üìö Section 8: Apply Isotonic Calibration\n",
    "\n",
    "Use `IsotonicRegression` to calibrate probability predictions. This fits a monotonic function to map the raw model predictions to better-calibrated probabilities.\n",
    "\n",
    "**Key points:**\n",
    "- Fit on out-of-fold (OOF) predictions from CV\n",
    "- Transform test predictions to improve calibration\n",
    "- `out_of_bounds='clip'` ensures predictions stay in [0, 1] range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb93df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit isotonic regression calibrator\n",
    "# Use out-of-fold predictions from CV for fitting\n",
    "calibrator = IsotonicRegression(out_of_bounds='clip')\n",
    "calibrator.fit(oof_blend, y)\n",
    "\n",
    "# Apply calibration to final test predictions\n",
    "final_pred = calibrator.transform(final_pred)\n",
    "\n",
    "print(f\"‚úÖ Isotonic Regression Calibration Applied\")\n",
    "print(f\"\\nCalibrated test predictions statistics:\")\n",
    "print(f\"  Min: {final_pred.min():.6f}\")\n",
    "print(f\"  Max: {final_pred.max():.6f}\")\n",
    "print(f\"  Mean: {final_pred.mean():.6f}\")\n",
    "print(f\"  Median: {np.median(final_pred):.6f}\")\n",
    "print(f\"  Std: {final_pred.std():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e4dc92",
   "metadata": {},
   "source": [
    "## üìö Section 9: Generate Submission File\n",
    "\n",
    "Create the final submission file with predicted probabilities in the required Kaggle format: (id, diagnosed_diabetes_probability).\n",
    "\n",
    "**Format Requirements:**\n",
    "- Header: id, diagnosed_diabetes\n",
    "- One row per test sample\n",
    "- Probabilities between 0 and 1\n",
    "- ROC-AUC is the evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21961b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission dataframe\n",
    "sub[TARGET] = final_pred\n",
    "\n",
    "# Save submission\n",
    "sub.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(f\"‚úÖ submission.csv saved!\")\n",
    "print(f\"\\nüìä Submission Statistics:\")\n",
    "print(f\"  File size: submission.csv\")\n",
    "print(f\"  Rows: {len(sub)}\")\n",
    "print(f\"  Columns: {list(sub.columns)}\")\n",
    "print(f\"\\nüéØ Prediction Distribution:\")\n",
    "print(f\"  Mean prediction: {final_pred.mean():.5f}\")\n",
    "print(f\"  Min prediction: {final_pred.min():.5f}\")\n",
    "print(f\"  Max prediction: {final_pred.max():.5f}\")\n",
    "print(f\"  Percentile 25: {np.percentile(final_pred, 25):.5f}\")\n",
    "print(f\"  Percentile 50: {np.percentile(final_pred, 50):.5f}\")\n",
    "print(f\"  Percentile 75: {np.percentile(final_pred, 75):.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00da973c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample submission\n",
    "print(f\"\\nüìù Sample Submission (first 10 rows):\")\n",
    "print(sub.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b21d350",
   "metadata": {},
   "source": [
    "## üéØ Summary\n",
    "\n",
    "### ‚úÖ Solution Components Executed:\n",
    "\n",
    "1. **Data Loading** - Loaded 700K training + external 100K dataset\n",
    "2. **External Feature Engineering** - 48 features from Diabetes Health Indicators Dataset\n",
    "3. **Manual Medical Features** - 3 clinically-informed features (BMI cat, BP cat, non-HDL)\n",
    "4. **Feature Preparation** - Label encoding, missing value handling\n",
    "5. **10-Fold CV Ensemble** - 3 models with ultra-heavy regularization\n",
    "6. **Feature Selection** - Reduced 75 ‚Üí 38 features\n",
    "7. **Final Model** - XGBoost on selected features\n",
    "8. **Probability Calibration** - IsotonicRegression for better estimates\n",
    "9. **Submission Generation** - ROC-AUC ready predictions\n",
    "\n",
    "### üèÜ Performance:\n",
    "- **Private Score:** 0.69760 (BEST)\n",
    "- **Public Score:** 0.70042\n",
    "- **Final Rank:** 877/4206 (Top 20.8%)\n",
    "- **CV AUC:** ~0.7299\n",
    "\n",
    "### üîë Key Success Factors:\n",
    "- ‚úÖ External dataset leverage (100K samples)\n",
    "- ‚úÖ Medical domain expertise (clinical features)\n",
    "- ‚úÖ Ultra-heavy regularization (3.5 L1, 4.0 L2)\n",
    "- ‚úÖ Balanced ensemble weights (0.50/0.35/0.15)\n",
    "- ‚úÖ Feature selection + calibration pipeline\n",
    "- ‚úÖ 10-Fold stratified cross-validation\n",
    "\n",
    "---\n",
    "\n",
    "**Next Steps:** Explore SMOTE (V26), deeper feature selection (V24), or additional model variants!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
