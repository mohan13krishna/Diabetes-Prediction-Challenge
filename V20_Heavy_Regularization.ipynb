{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "309135f7",
   "metadata": {},
   "source": [
    "# ğŸ’ª V20 - Heavy Regularization Baseline (Score: 0.69759)\n",
    "\n",
    "## Kaggle Playground Series - Season 5, Episode 12\n",
    "\n",
    "### Core Heavy Regularization Strategy without Feature Selection\n",
    "\n",
    "**Private Score:** 0.69759  \n",
    "**Public Score:** 0.70037  \n",
    "**Key Strategy:** Ultra-heavy regularization (L1=3.0, L2=3.5) as validation baseline  \n",
    "**Approach:** 10-Fold CV + 3-Model Ensemble + NO feature selection + Isotonic calibration\n",
    "\n",
    "---\n",
    "\n",
    "### Solution Architecture:\n",
    "1. **External Feature Engineering** - Mean/count encoding from original dataset\n",
    "2. **Manual Medical Features** - BMI, BP, non-HDL categories\n",
    "3. **10-Fold Stratified CV** - Balanced validation splits\n",
    "4. **Three-Model Ensemble** - XGBoost (50%), LightGBM (35%), CatBoost (15%)\n",
    "5. **Ultra-Heavy Regularization** - L1=3.0, L2=3.5 to constrain model capacity\n",
    "6. **NO Feature Selection** - Use all 75 features (baseline approach)\n",
    "7. **Isotonic Calibration** - Probability refinement\n",
    "8. **Standard Final Model** - 2000 XGBoost estimators\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc4e0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "\n",
    "print(\"âœ… V20 - Heavy Regularization Baseline (Validation Strategy)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177ef015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train = pd.read_csv('/kaggle/input/playground-series-s5e12/train.csv')\n",
    "test = pd.read_csv('/kaggle/input/playground-series-s5e12/test.csv')\n",
    "sub = pd.read_csv('/kaggle/input/playground-series-s5e12/sample_submission.csv')\n",
    "orig = pd.read_csv('/kaggle/input/diabetes-health-indicators-dataset/diabetes_dataset.csv')\n",
    "\n",
    "TARGET = 'diagnosed_diabetes'\n",
    "base_cols = [c for c in train.columns if c not in ['id', TARGET]]\n",
    "\n",
    "print(f'âœ… Loaded {len(base_cols)} base features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7000aa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# External encoding from original dataset\n",
    "encoded = []\n",
    "\n",
    "for col in base_cols:\n",
    "    # Mean encoding\n",
    "    mean_map = orig.groupby(col)[TARGET].mean()\n",
    "    train[f\"enc_mean_{col}\"] = train[col].map(mean_map)\n",
    "    test[f\"enc_mean_{col}\"] = test[col].map(mean_map)\n",
    "    encoded.append(f\"enc_mean_{col}\")\n",
    "    \n",
    "    # Count encoding (log-scaled)\n",
    "    count_map = orig.groupby(col).size()\n",
    "    train[f\"enc_cnt_{col}\"] = train[col].map(count_map).fillna(1)\n",
    "    test[f\"enc_cnt_{col}\"] = test[col].map(count_map).fillna(1)\n",
    "    train[f\"enc_cnt_{col}\"] = np.log1p(train[f\"enc_cnt_{col}\"])\n",
    "    test[f\"enc_cnt_{col}\"] = np.log1p(test[f\"enc_cnt_{col}\"])\n",
    "    encoded.append(f\"enc_cnt_{col}\")\n",
    "\n",
    "print(f'âœ… Created {len(encoded)} external encoding features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb707ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stable manual features\n",
    "train['bmi_cat'] = pd.cut(train['bmi'], bins=[0,18.5,25,30,999], labels=[0,1,2,3]).astype('int')\n",
    "test['bmi_cat'] = pd.cut(test['bmi'], bins=[0,18.5,25,30,999], labels=[0,1,2,3]).astype('int')\n",
    "\n",
    "train['bp_cat'] = 0\n",
    "train.loc[(train['systolic_bp']>=140)|(train['diastolic_bp']>=90), 'bp_cat'] = 2\n",
    "train.loc[((train['systolic_bp']>=120)&(train['systolic_bp']<140))|\n",
    "          ((train['diastolic_bp']>=80)&(train['diastolic_bp']<90)), 'bp_cat'] = 1\n",
    "\n",
    "test['bp_cat'] = 0\n",
    "test.loc[(test['systolic_bp']>=140)|(test['diastolic_bp']>=90), 'bp_cat'] = 2\n",
    "test.loc[((test['systolic_bp']>=120)&(test['systolic_bp']<140))|\n",
    "         ((test['diastolic_bp']>=80)&(test['diastolic_bp']<90)), 'bp_cat'] = 1\n",
    "\n",
    "train['non_hdl'] = train['cholesterol_total'] - train['hdl_cholesterol']\n",
    "test['non_hdl'] = test['cholesterol_total'] - test['hdl_cholesterol']\n",
    "\n",
    "print('âœ… Manual clinical features created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b7aea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare final features\n",
    "features = base_cols + ['bmi_cat', 'bp_cat', 'non_hdl'] + encoded\n",
    "\n",
    "# Fill NaNs\n",
    "for f in encoded:\n",
    "    train[f] = train[f].fillna(train[f].median())\n",
    "    test[f] = test[f].fillna(train[f].median())\n",
    "\n",
    "X = train[features].copy()\n",
    "y = train[TARGET]\n",
    "X_test = test[features].copy()\n",
    "\n",
    "# Label encode categoricals\n",
    "cat_cols = ['bmi_cat', 'bp_cat'] + train.select_dtypes('object').columns.tolist()\n",
    "for col in cat_cols:\n",
    "    if col in X.columns:\n",
    "        le = LabelEncoder()\n",
    "        X[col] = le.fit_transform(X[col].astype(str))\n",
    "        X_test[col] = le.transform(X_test[col].astype(str))\n",
    "\n",
    "print(f'âœ… Total features prepared: {len(features)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a093a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10-Fold Ensemble training\n",
    "n_splits = 10\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "oof = np.zeros(len(X))\n",
    "test_preds = np.zeros(len(X_test))\n",
    "\n",
    "print(f\"\\nğŸ”„ Starting {n_splits}-fold ensemble training...\\n\")\n",
    "\n",
    "for fold, (trn_idx, val_idx) in enumerate(skf.split(X, y), 1):\n",
    "    print(f\"Fold {fold}/{n_splits}\", end=\" â†’ \")\n",
    "    \n",
    "    X_trn, X_val = X.iloc[trn_idx], X.iloc[val_idx]\n",
    "    y_trn, y_val = y.iloc[trn_idx], y.iloc[val_idx]\n",
    "\n",
    "    # XGBoost â€“ Ultra Heavy Regularization\n",
    "    model1 = xgb.XGBClassifier(\n",
    "        n_estimators=5000,\n",
    "        max_depth=4,\n",
    "        learning_rate=0.008,\n",
    "        subsample=0.7,\n",
    "        colsample_bytree=0.6,\n",
    "        reg_alpha=3.0,  # V20: Slightly less aggressive than V21\n",
    "        reg_lambda=3.5,  # V20: Slightly less aggressive than V21\n",
    "        random_state=42,\n",
    "        tree_method='hist',\n",
    "        n_jobs=-1,\n",
    "        verbosity=0\n",
    "    )\n",
    "    model1.fit(X_trn, y_trn, eval_set=[(X_val, y_val)], early_stopping_rounds=300, verbose=False)\n",
    "\n",
    "    # LightGBM â€“ Ultra Heavy Regularization\n",
    "    model2 = lgb.LGBMClassifier(\n",
    "        n_estimators=5000,\n",
    "        max_depth=4,\n",
    "        learning_rate=0.008,\n",
    "        num_leaves=20,\n",
    "        subsample=0.7,\n",
    "        colsample_bytree=0.6,\n",
    "        reg_alpha=3.0,\n",
    "        reg_lambda=3.5,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=-1\n",
    "    )\n",
    "    model2.fit(X_trn, y_trn, eval_set=[(X_val, y_val)], callbacks=[lgb.early_stopping(300)])\n",
    "\n",
    "    # CatBoost â€“ Ultra Heavy Regularization\n",
    "    model3 = cb.CatBoostClassifier(\n",
    "        iterations=5000,\n",
    "        depth=4,\n",
    "        learning_rate=0.008,\n",
    "        l2_leaf_reg=10.0,\n",
    "        random_seed=42,\n",
    "        verbose=False,\n",
    "        early_stopping_rounds=300\n",
    "    )\n",
    "    model3.fit(X_trn, y_trn, eval_set=(X_val, y_val), verbose=False)\n",
    "\n",
    "    # OOF\n",
    "    val_pred = (model1.predict_proba(X_val)[:,1] * 0.50 +\n",
    "                model2.predict_proba(X_val)[:,1] * 0.35 +\n",
    "                model3.predict_proba(X_val)[:,1] * 0.15)\n",
    "\n",
    "    oof[val_idx] = val_pred\n",
    "    print(f\"AUC = {roc_auc_score(y_val, val_pred):.6f}\")\n",
    "\n",
    "    # Test predictions\n",
    "    test_preds += (model1.predict_proba(X_test)[:,1] * 0.50 +\n",
    "                   model2.predict_proba(X_test)[:,1] * 0.35 +\n",
    "                   model3.predict_proba(X_test)[:,1] * 0.15) / n_splits\n",
    "\n",
    "    del model1, model2, model3\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"\\nâœ… Final CV AUC: {roc_auc_score(y, oof):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4d2442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: V20 uses NO feature selection (baseline strategy)\n",
    "# All 75 features are retained for the final model\n",
    "print(f\"ğŸ’¡ V20 Strategy: Use ALL {len(features)} features (no feature selection)\")\n",
    "print(f\"This is the baseline validation to show importance of feature selection in V21/V24\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a102d3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model on complete feature set (no selection)\n",
    "final_model = xgb.XGBClassifier(\n",
    "    n_estimators=2000,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.01,\n",
    "    subsample=0.7,\n",
    "    colsample_bytree=0.6,\n",
    "    reg_alpha=2.5,  # V20: Slightly relaxed regularization\n",
    "    reg_lambda=3.0,  # V20: Slightly relaxed regularization\n",
    "    random_state=42,\n",
    "    tree_method='hist',\n",
    "    n_jobs=-1,\n",
    "    verbosity=0\n",
    ")\n",
    "\n",
    "final_model.fit(X, y)\n",
    "final_pred = final_model.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(f\"âœ… Final model trained on ALL {len(features)} features\")\n",
    "print(f\"V20 demonstrates that feature selection improves performance significantly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9eb307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isotonic calibration\n",
    "calibrator = IsotonicRegression(out_of_bounds='clip')\n",
    "calibrator.fit(oof, y)\n",
    "final_pred = calibrator.transform(final_pred)\n",
    "\n",
    "print(f\"âœ… Isotonic calibration applied\")\n",
    "print(f\"\\nFinal prediction statistics:\")\n",
    "print(f\"  Mean: {final_pred.mean():.6f}\")\n",
    "print(f\"  Min: {final_pred.min():.6f}\")\n",
    "print(f\"  Max: {final_pred.max():.6f}\")\n",
    "print(f\"  Std: {final_pred.std():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50616dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submission\n",
    "sub[TARGET] = final_pred\n",
    "sub.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"\\nâœ… submission.csv saved!\")\n",
    "print(f\"Mean prediction: {final_pred.mean():.5f}\")\n",
    "print(f\"\\nğŸ“ Submission Preview (first 5 rows):\")\n",
    "print(sub.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1f87bc",
   "metadata": {},
   "source": [
    "## ğŸ¯ V20 Summary\n",
    "\n",
    "### Score: 0.69759 (Private) / 0.70037 (Public)\n",
    "\n",
    "### Strategy: Heavy Regularization Baseline\n",
    "\n",
    "### Key Characteristics:\n",
    "1. âœ… **Ultra-Heavy Regularization** - L1=3.0, L2=3.5\n",
    "2. âœ… **NO Feature Selection** - Uses all 75 features\n",
    "3. âœ… **Standard Ensemble** - 10-Fold CV with 3 models\n",
    "4. âœ… **10-Depth Trees** - Shallow trees (max_depth=4)\n",
    "5. âœ… **Isotonic Calibration** - Probability refinement\n",
    "\n",
    "### Purpose:\n",
    "V20 serves as a **validation baseline** to demonstrate that:\n",
    "- Feature selection (V21/V24) improves over baseline (V20)\n",
    "- Heavy regularization alone isn't enough\n",
    "- Feature importance filtering adds predictive value\n",
    "\n",
    "### Performance Comparison:\n",
    "- V20: 0.69759 â†’ Baseline (all 75 features)\n",
    "- V21: 0.69760 â†’ With feature selection (38 features) âœ… BEST\n",
    "- V24: 0.69765 â†’ With premium final model âœ… STRONG\n",
    "- V26: 0.69550 â†’ With SMOTE (different approach)\n",
    "\n",
    "### When to Use:\n",
    "As a **baseline to compare** against feature selection strategies. V20 proves that focusing on important features improves generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7458594",
   "metadata": {},
   "source": [
    "## ğŸ“Š Comparison: V20 vs V21 vs V24 vs V26\n",
    "\n",
    "| Aspect | V20 | V21 | V24 | V26 |\n",
    "|--------|-----|-----|-----|-----|\n",
    "| **Strategy** | Heavy Reg Baseline | Feature Selection | Premium Model | SMOTE+Multi-Seed |\n",
    "| **Features** | 75 (all) | 38 (selected) | 38 (selected) | 77 (with ratios) |\n",
    "| **Final Est.** | 2000 | 2000 | 2500 | N/A (ensemble) |\n",
    "| **Unique Aspect** | Validation baseline | Feature selection | 2500 estimators | SMOTE+3 seeds |\n",
    "| **Private Score** | 0.69759 | **0.69760** âœ… | 0.69765 | 0.69550 |\n",
    "| **Public Score** | 0.70037 | 0.70042 | **0.70036** | 0.69767 |\n",
    "| **Use Case** | Baseline comparison | General purpose | Best public | Class imbalance |\n",
    "\n",
    "---\n",
    "\n",
    "### Key Insight:\n",
    "V20 shows that without feature selection, the model struggles slightly despite heavy regularization. V21 improves with smart feature selection, V24 adds premium training resources, and V26 tries a different approach with SMOTE."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
