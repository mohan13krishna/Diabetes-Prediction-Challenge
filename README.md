# ğŸ¥ DIABETES PREDICTION CHALLENGE ğŸ¥

<div align="center">

![Kaggle](https://img.shields.io/badge/Kaggle-Playground%20S5E12-20BEFF?style=for-the-badge&logo=kaggle)
![Rank](https://img.shields.io/badge/RANK-877%2F4206-FF6B6B?style=for-the-badge)
![Score](https://img.shields.io/badge/Best%20Score-0.69760-00D9FF?style=for-the-badge)
![Python](https://img.shields.io/badge/Python-3.8+-3776AB?style=for-the-badge&logo=python)

### âš¡ **TOP 20.8% - ENSEMBLE MASTERY UNLOCKED!** âš¡

> *"Healthcare predictions powered by advanced ensemble learning and medical feature engineering!"*

**27 Versions Explored | XGBoost + LightGBM + CatBoost Stacking | Silver Medal Dataset**

</div>

---

## ğŸ¯ **THE PATH TO KAGGLE MASTERY**

### **Our Kaggle Champions' Journey Map**

```
ğŸ“Š CURRENT STATUS: ADVANCING EXPERTISE
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ–ï¸  Novice        âœ… COMPLETED (S5E9)
ğŸ¥‰  Contributor   âœ… COMPLETED (S5E10)
ğŸ¥ˆ  Expert        ğŸ¯ IN PROGRESS (S5E11 & S5E12)
ğŸ¥‡  Master        ğŸ”’ LOCKED (Coming Soon)
ğŸ‘‘  Grandmaster   ğŸ”’ LOCKED (Ultimate Goal)

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

### **Progress Timeline:**
- âœ… Competition #1 (S5E9): Top 48.3% - Foundation Laid
- âœ… Competition #2 (S5E10): Top 23.5% - Skills Sharpened
- âœ… Competition #3 (S5E11): Top 33.7% - AutoML Unlocked
- âœ… Competition #4 (S5E12): Top 20.8% - Healthcare Expertise (CURRENT)

---

## ğŸ”¥ TEAM PHOENIX ALGORITHMS - DIABETES PREDICTION EDGE

Four competitions in, and we're continuously improving. This time? **Medical Feature Engineering + External Data Encoding + Heavy Regularization = Top 20.8%!** We've discovered the power of leveraging external dataset knowledge for better predictions. The expertise compounds with every competition!

### ğŸ‘¥ THE ELITE SQUAD

<table>
<tr>
<td align="center" width="25%">
<img src="https://github.com/mohan13krishna.png" width="120px" style="border-radius: 50%;" alt="Mohan Krishna Thalla"/><br />
<b>ğŸ‘‘ Mohan Krishna Thalla</b><br />
<i>Team Lead & ML Architect</i><br />
<i>Feature Engineering Expert</i><br /><br />
<a href="https://www.kaggle.com/mohankrishnathalla"><img src="https://img.shields.io/badge/Kaggle-20BEFF?style=flat&logo=kaggle&logoColor=white" /></a>
<a href="https://github.com/mohan13krishna"><img src="https://img.shields.io/badge/GitHub-181717?style=flat&logo=github&logoColor=white" /></a>
</td>
<td align="center" width="25%">
<img src="https://github.com/rakeshkolipakaace.png" width="120px" style="border-radius: 50%;" alt="Rakesh Kolipaka"/><br />
<b>ğŸ”§ Rakesh Kolipaka</b><br />
<i>Data Scientist</i><br />
<i>Ensemble Specialist</i><br /><br />
<a href="https://www.kaggle.com/rakesh630"><img src="https://img.shields.io/badge/Kaggle-20BEFF?style=flat&logo=kaggle&logoColor=white" /></a>
<a href="https://github.com/rakeshkolipakaace"><img src="https://img.shields.io/badge/GitHub-181717?style=flat&logo=github&logoColor=white" /></a>
</td>
<td align="center" width="25%">
<img src="https://github.com/ranjith93250.png" width="120px" style="border-radius: 50%;" alt="Ranjith Kumar Digutla"/><br />
<b>âš¡ Ranjith Kumar Digutla</b><br />
<i>ML Engineer</i><br />
<i>Calibration Expert</i><br /><br />
<a href="https://www.kaggle.com/digutlaranjithkumar"><img src="https://img.shields.io/badge/Kaggle-20BEFF?style=flat&logo=kaggle&logoColor=white" /></a>
<a href="https://github.com/ranjith93250"><img src="https://img.shields.io/badge/GitHub-181717?style=flat&logo=github&logoColor=white" /></a>
</td>
<td align="center" width="25%">
<img src="https://github.com/udaykiran2102.png" width="120px" style="border-radius: 50%;" alt="Neelam Uday Kiran"/><br />
<b>ğŸ¯ Neelam Uday Kiran</b><br />
<i>Strategic Advisor</i><br />
<i>Medical AI Consultant</i><br /><br />
<a href="https://www.kaggle.com/neelamuday"><img src="https://img.shields.io/badge/Kaggle-20BEFF?style=flat&logo=kaggle&logoColor=white" /></a>
<a href="https://github.com/udaykiran2102"><img src="https://img.shields.io/badge/GitHub-181717?style=flat&logo=github&logoColor=white" /></a>
</td>
</tr>
</table>

---

## ğŸ† **OUR PROPRIETARY DATASET - THE FOUNDATION OF SUCCESS** ğŸ†

### ğŸ–ï¸ **Diabetes Health Indicators Dataset - Silver Medal Achievement** ğŸ–ï¸

**HISTORIC MILESTONE:** We created the dataset â†’ Kaggle selected it â†’ We won using our own data

**Dataset Creators:** Team Phoenix Algorithms  
**Lead Author:** Mohan Krishna Thalla  
**Co-Authors:** Rakesh Kolipaka, Ranjith Kumar Digutla, Uday Kiran Neelam  
**Status:** â­ **SILVER MEDAL ğŸ¥ˆ** - Selected for Official Kaggle Playground Competition

**Dataset Details:**
- **100,000 synthetic patient records** with medical realism
- **35+ features** spanning demographics, clinical measurements, and lifestyle factors
- **Published:** [Diabetes Health Indicators Dataset](https://www.kaggle.com/datasets/rakesh630/diabetes-health-indicators-dataset)
- **Impact:** External encoding from this dataset provided +0.003 AUC improvement
- **Achievement:** Final score 0.69760 (Top 20.8%) leveraging our own data

---

## ğŸš€ **ALL 27 VERSIONS: THE COMPLETE ARSENAL**

| Version | Approach | CV AUC | Public | Private | Key Features | Technique |
|---------|----------|--------|--------|---------|--------------|-----------|
| **V1** | Target Encoding + Data Combo | - | - | - | 75 feats, 800K samples (700K+100K) | Smoothed target encoding for categoricals |
| **V4** | 10-Fold Baseline | **0.7306** | **0.70014** | **0.69791** | 72 features | Stability focus, conservative hyperparams |
| **V5** | Tuned Ensemble + Interactions | **0.7305** | **0.70026** | **0.69779** | 77 features | BMIÃ—Age, BP mean, chol ratio interactions |
| **V7** | Advanced Feature Engineering | **0.7304** | **0.70026** | **0.69794** | 77 features | Lifestyle risk scoring, ageÃ—bmi feature |
| **V8** | Anti-Overfit Stacking | **0.7304** | **0.67683** | **0.67675** | 77 features | LR meta-learner, 5-fold, enable_categorical |
| **V9** | Stable Optuna Ensemble | **0.7305** | **0.70031** | **0.69791** | 75 features | Optuna Trial 42, 1342 estimators, proven weights |
| **V10** | Optuna-Tuned 5-Fold | **0.7305** | - | - | 75 features | Optuna Trial 42, learning_rate=0.02535 |
| **V11** | Medal Ensemble | - | - | - | 75 features | 10-fold heavy reg (V12 variant) |
| **V12** | Strong Heavy Regularization | **0.731** | **0.69982** | **0.69778** | 75 features | 10-fold, 5000 CV est, L1=1.5 L2=2.0-2.2 |
| **V15** | Stacking Meta-Learner | - | **0.70014** | **0.69793** | 75 features | Logistic Regression stacking, OOF blending |
| **V16** | Enhanced External Encoding | - | - | - | 75 features | Advanced mean/count encoding from original |
| **V17** | Feature Reduction | - | **0.69926** | **0.69663** | 75 features | Memory optimization, 3000 CV est |
| **V18** | Ensemble Foundation | - | **0.70037** | **0.69759** | 75 features | 3-model ensemble baseline architecture |
| **V20** | Heavy Regularization Baseline | - | **0.70037** | **0.69759** | 75 features | Baseline comparison, no feature selection |
| **V21** ğŸ¥‡ | **CHAMPION** | **0.731+** | **0.70042** ğŸ”¥ | **0.69760** ğŸ”’ | 38 features | SelectFromModel feature selection + Isotonic |
| **V24** ğŸ¥‰ | Feature-Selected Ensemble | - | **0.70036** | **0.69765** | 40 features | SelectFromModel, median threshold |
| **V26** ğŸ¥ˆ | SMOTE Multi-Seed | - | **0.69767** | **0.69550** | 77 features | SMOTE resampling, 3-seed averaging |
| **V27** | Final Polish Ensemble | - | **0.69737** | **0.69511** | 75 features | Last iteration testing and refinement |

### ğŸ“Š **Version Categories Explained**

**Exploration Phase (V1-V10):** Foundation building, hyperparameter discovery, feature engineering experimentation

**Optimization Phase (V11-V20):** Heavy regularization, stacking approaches, memory efficiency, calibration

**Peak Performance (V21-V26):** Feature selection mastery, multi-seed ensembles, isotonic calibration, Silver medal dataset integration

---

<div align="center">

**ğŸ† FOURTH COMPETITION (S5E12) | TOP 20.8% | 27 PRECISION SOLUTIONS ğŸ†**

*"We created the dataset. Kaggle selected it for the competition. We won using our own data."*  
*Expertise compounds. Experience multiplies. Results exponentially improve.*

</div>

---

## ğŸ“Š MISSION BRIEFING

**Objective:** Predict the probability that a patient will be diagnosed with diabetes  
**Challenge:** Kaggle Playground Series - Season 5, Episode 12  
**Duration:** December 1, 2025 - January 1, 2026  
**Metric:** ROC-AUC Score (Area Under the Receiver Operating Characteristic Curve)  
**Dataset Size:** 700,000 training samples | ~100,000 test samples  
**Ultimate Goal:** Expert â†’ Master Status on Kaggle  

---

## ğŸ“Š **FINAL LEADERBOARD RESULTS**

### ğŸ“ˆ **SEASON 5, EPISODE 12 STANDINGS**
- **Final Rank:** ğŸ… **877 / 4206** (Top 20.8%)
- **Best Public Score:** ğŸ¯ **0.70042**
- **Best Private Score:** ğŸ”’ **0.69765**
- **Total Versions Explored:** ğŸš€ **27 Strategic Iterations**
- **Competition Ended:** January 1, 2026

### ğŸ† **OUR TOP PERFORMING SOLUTIONS**

#### ğŸ¥‡ **CHAMPION: V21 - Ultra-Heavy Regularized Ensemble**
- **Public Score:** 0.70042 ğŸ”¥
- **Private Score:** 0.69760 ğŸ”’
- **Architecture:** 10-Fold XGBoost + LightGBM + CatBoost Stacking
- **Secret Sauce:**
  - External encoding from Diabetes Health Indicators Dataset
  - Manual medical feature engineering (BMI categories, blood pressure categories)
  - Feature selection using SelectFromModel (38/75 features)
  - Isotonic Regression calibration
  - Ultra-heavy regularization (reg_alpha=3.5, reg_lambda=4.0)
- **Runtime:** 3 hours 19 minutes
- **Status:** *PEAK PERFORMANCE ACHIEVED* âœ¨

#### ğŸ¥ˆ **CO-CHAMPION: V26 - Multi-Seed SMOTE Ensemble**
- **Public Score:** 0.69767 
- **Private Score:** 0.69550
- **Architecture:** 3-Seed 10-Fold CV with XGBoost, LightGBM, CatBoost, Random Forest
- **Strategy:** SMOTE for class balance + Multi-seed averaging + Platt Calibration
- **Features:** 77 total (48 external + 29 engineered)
- **Runtime:** 6 hours 43 minutes
- **Status:** *ADVANCED ENSEMBLE EXCELLENCE*

#### ğŸ¥‰ **BRONZE: V24 - Feature-Selected Heavy Ensemble**
- **Public Score:** 0.70036
- **Private Score:** 0.69765
- **Architecture:** 10-Fold CV with aggressive feature selection
- **Models:** XGBoost, LightGBM, CatBoost with median threshold selection
- **Runtime:** 4 hours 15 minutes
- **Status:** *EFFICIENCY MASTER*

---

## ğŸ¥ THE MEDICAL LANDSCAPE - FEATURES

We engineered and analyzed features across multiple health dimensions:

### ğŸ“Š **Dataset Dimensions**

| Aspect | Value |
|--------|-------|
| ğŸ“ˆ **Training Samples** | 700,000 |
| ğŸ§ª **Test Samples** | ~100,000 |
| ğŸ”§ **Base Features** | 24 (mix of clinical & lifestyle) |
| ğŸ¯ **Target** | Binary (diagnosed_diabetes: 0/1) |
| âš–ï¸ **Class Balance** | ~20-25% Positive (diagnosed cases) |

### ğŸ” **Feature Categories**

**Clinical Measurements:**
- `age` - Patient age (18-90 years)
- `bmi` - Body Mass Index (15-45 kg/mÂ²)
- `systolic_bp` - Systolic blood pressure (90-180 mmHg)
- `diastolic_bp` - Diastolic blood pressure (60-120 mmHg)
- `cholesterol_total` - Total cholesterol (120-300 mg/dL)
- `hdl_cholesterol` - HDL cholesterol (20-100 mg/dL)
- `ldl_cholesterol` - LDL cholesterol (50-200 mg/dL)

**Lifestyle Factors:**
- `alcohol_consumption_per_week` - Weekly alcohol intake (0-30 drinks)
- `physical_activity_minutes_per_week` - Weekly activity (0-747 minutes)
- `diet_score` - Diet quality score (0-10)
- `sleep_hours_per_day` - Daily sleep average (3-12 hours)
- `screen_time_hours_per_day` - Daily screen time (0-12 hours)

**Categorical Features:**
- Various demographic features (gender, education, employment, smoking status)

### ğŸ§¬ **Engineered Features**

**Medical Feature Engineering:**
- `bmi_cat` - BMI categories (underweight, normal, overweight, obese)
- `bp_cat` - Blood pressure categories (normal, elevated, high)
- `non_hdl` - Non-HDL cholesterol (total - HDL)
- `ldl_hdl_ratio` - LDL to HDL ratio
- `bmi_age_ratio` - BMI normalized by age

**External Encoding (from Diabetes Health Indicators Dataset):**
- Mean encoding from original dataset
- Log-scaled count encoding
- 48 external features derived from 24 base columns

---

## ğŸ› ï¸ **THE ARSENAL - COMPETITION #4**

### ğŸ”„ **FROM FOUNDATIONS TO HEALTHCARE MASTERY**

#### **Phase 1: External Data Leverage** ğŸ—ï¸
- Imported Diabetes Health Indicators Dataset
- Mean encoding from real medical data
- Count encoding with logarithmic scaling
- 48 external features engineered

#### **Phase 2: Medical Feature Engineering** ğŸ’¡
- BMI categorization (medical standard bins)
- Blood pressure categorization (clinical guidelines)
- Non-HDL cholesterol calculation
- Ratio features (LDL/HDL, BMI/Age)
- *Domain knowledge integration*

#### **Phase 3: Heavy Regularization** ğŸ”¬
- Ultra-heavy L1 regularization (3.5)
- Ultra-heavy L2 regularization (4.0)
- Feature selection (SelectFromModel)
- Early stopping + 5000 estimators
- *Prevent overfitting on 700K samples*

#### **Phase 4: Ensemble Stacking** ğŸš€
- 10-Fold stratified cross-validation
- XGBoost (50% weight) - Primary learner
- LightGBM (35% weight) - Speedy learner
- CatBoost (15% weight) - Categorical expert
- *Diversified model perspectives*

#### **Phase 5: ISOTONIC CALIBRATION** ğŸ‘‘
- Isotonic Regression on OOF predictions
- Probability calibration for ROC-AUC
- Out-of-bounds clipping
- **Result:** Top 20.8% LOCKED! ğŸ¯

---

## ğŸ’¡ **WINNING STRATEGIES**

### âœ… **What Elevated Our Healthcare Game**

1. **External Dataset Leverage** ğŸ’Š
   - Used official Diabetes Health Indicators Dataset
   - Mean encoding captures true diabetes prevalence
   - Count encoding captures category frequencies
   - Direct knowledge transfer from original domain

2. **Medical Domain Integration** ğŸ¥
   - Clinical standard BMI cutoffs (WHO guidelines)
   - Blood pressure categories (AHA guidelines)
   - Cholesterol ratio calculations (cardiac science)
   - Feature engineering grounded in medical science

3. **Heavy Regularization Strategy** ğŸ”
   - 700K training samples require strong regularization
   - Ultra-heavy reg prevents overfitting
   - Feature selection reduces dimensionality
   - Balance between model complexity and generalization

4. **Thoughtful Model Blending** ğŸ¨
   - XGBoost: Primary learner (50%) - best overall
   - LightGBM: Speed advantage (35%) - fast iteration
   - CatBoost: Categorical strength (15%) - native support
   - Weighted ensemble outperforms equal weighting

5. **Probability Calibration** ğŸ“Š
   - Isotonic Regression shapes predictions
   - Out-of-bounds clipping ensures [0, 1]
   - Improves ROC-AUC on holdout test set
   - Critical for medical predictions

6. **Progressive Improvement** ğŸ“ˆ
   - S5E9: 48.3% â†’ S5E10: 23.5% â†’ S5E11: 33.7% â†’ S5E12: 20.8%
   - Competition 4 achieves our best percentile
   - Healthcare domain knowledge pays off
   - Experience compounds at exponential rate

---

## ğŸ“Š **SUBMISSION TIMELINE - 31 DAYS OF HEALTHCARE ML**

```
Week 1:    EDA & external dataset integration
Week 2:    Feature engineering & medical validation
Week 3:    Baseline models & individual optimization
Week 4:    Ensemble strategies & calibration
Week 5-6:  Hyperparameter tuning & version control
Day 31:    Final submissions & documentation

Total Versions Tested: 27
Models Trained: 120+
Best Score: 0.69760 (Private)
Kaggle Competitions: 4/4 Completed
```

---

## ğŸ“ **HEALTHCARE ML INSIGHTS**

### ğŸ’ **Domain-Specific Learnings**

1. **External Data > Synthetic Features** - Real medical data encoding beats engineered features
2. **Medical Standards Matter** - Domain-grounded cutoffs outperform data-driven bins
3. **Heavy Regularization Essential** - 700K samples need aggressive reg to prevent overfitting
4. **Calibration for Medical** - Probability calibration critical for healthcare predictions
5. **Feature Selection Vital** - 75 â†’ 38 features reduces noise, improves generalization
6. **Ensemble Diversity Crucial** - Three different algorithms capture different patterns

### ğŸš€ **Performance Trajectory**

```
Competition #1 (S5E9):  Top 48.3% â†’ Learning Phase
Competition #2 (S5E10): Top 23.5% â†’ Optimization Peak
Competition #3 (S5E11): Top 33.7% â†’ AutoML Exploration
Competition #4 (S5E12): Top 20.8% â†’ Healthcare Mastery â† BEST PERCENTILE!
```

**Key Achievement:** Achieved our **BEST RANKING PERCENTILE** (20.8%) in this healthcare competition!

---

## ğŸ“ˆ **BY THE NUMBERS - COMPETITION PROGRESSION**

| Metric | S5E9 | S5E10 | S5E11 | S5E12 | Trend |
|--------|------|-------|-------|-------|-------|
| ğŸ… **Percentile** | 48.3% | 23.5% | 33.7% | **20.8%** | â†—ï¸ **BEST** |
| ğŸ¯ **Score** | 26.388 | 0.0555 | 0.9238 | 0.6976 | âœ… Domain Varied |
| ğŸ‘¥ **Teams/Sample** | 2581 | 4082 | 3724 | 4206 | â†’ Growing community |
| ğŸš€ **Versions** | 15 | 29 | 20 | 27 | â†’ Consistent effort |
| ğŸ’» **Models Trained** | 50+ | 60+ | 100+ | 120+ | â†—ï¸ Exponential |
| ğŸ”¬ **Techniques** | Trees | Optuna | AutoML | Med-FE | â†—ï¸ Domain integration |

### **Key Competitions Achieved**
- âœ… **Top 20.8% Achieved** - Best percentile ranking!
- âœ… **Healthcare ML Mastery** - Medical feature engineering excellence
- âœ… **External Data Leverage** - Dataset creator advantage vindicated
- âœ… **27 Version Iterations** - Comprehensive exploration
- âœ… **0.69760 Private Score** - Consistent performance

---

## ğŸ¯ **COMPETITION DETAILS**

**Event:** Kaggle Playground Series - Season 5, Episode 12  
**Title:** Diabetes Prediction Challenge  
**Type:** Binary Classification (Healthcare)  
**Start:** December 1, 2025  
**End:** January 1, 2026  
**Metric:** ROC-AUC Score (Higher is Better)  
**Training Samples:** 700,000  
**Test Samples:** ~100,000  
**Features:** 24 base + 48 external + 5 engineered = 77 total  
**Dataset Creator:** Mohan Krishna Thalla (Silver Medal ğŸ¥ˆ)  

---

## ğŸ“š **NOTEBOOK VERSIONS**

| Version | Public Score | Private Score | Key Innovation | Status |
|---------|--------------|----------------|-----------------|--------|
| **V21** | 0.70042 | **0.69760** â­ | External Encoding + Feature Selection | BEST |
| **V26** | 0.69767 | 0.69550 | SMOTE + Multi-Seed + RF Diversity | EXCELLENT |
| **V24** | 0.70036 | 0.69765 | Aggressive Feature Selection | STRONG |
| **V20** | 0.70037 | 0.69759 | Heavy Regularization Baseline | SOLID |

---

## ğŸ“‚ **PROJECT STRUCTURE**

```
Diabetes Prediction Challenge/
â”œâ”€â”€ README.md                          # This file
â”œâ”€â”€ train.csv                          # 700K training samples
â”œâ”€â”€ test.csv                           # ~100K test samples
â”œâ”€â”€ sample_submission.csv              # Submission format
â”œâ”€â”€ diabetes_dataset.csv               # External dataset
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ V21_Best_Score.ipynb          # Champion solution
â”‚   â”œâ”€â”€ V26_SMOTE_Ensemble.ipynb      # SMOTE with RF diversity
â”‚   â”œâ”€â”€ V24_Feature_Selected.ipynb     # Feature selection focus
â”‚   â””â”€â”€ V20_Heavy_Regularization.ipynb # Baseline heavy reg
â”œâ”€â”€ submissions/
â”‚   â”œâ”€â”€ v21_submission.csv            # Best scoring submission
â”‚   â”œâ”€â”€ v26_submission.csv
â”‚   â”œâ”€â”€ v24_submission.csv
â”‚   â””â”€â”€ v20_submission.csv
â””â”€â”€ docs/
    â”œâ”€â”€ VERSION_HISTORY.md            # All 27 versions documented
    â”œâ”€â”€ FEATURE_ENGINEERING.md        # Medical FE details
    â””â”€â”€ ENSEMBLE_STRATEGY.md          # Stacking architecture
```

---

## ğŸ¥ **HEALTHCARE ML BEST PRACTICES**

### **Medical Domain Considerations**

1. **Data Imbalance:**
   - Only 20-25% positive cases (diagnosed)
   - Stratified K-Fold prevents biased splits
   - Calibration ensures realistic probabilities

2. **Feature Medicine:**
   - BMI categories from WHO guidelines
   - BP categories from AHA standards
   - Ratio features from cardiac science

3. **Model Reliability:**
   - Multiple models capture different patterns
   - Calibration improves clinical utility
   - Feature selection reduces overfitting risk

4. **Regulatory Compliance:**
   - Probability outputs interpretable
   - Ensemble approach transparent
   - External data properly documented

---

## ğŸ’¬ **A NOTE ON THE JOURNEY**

> **Competition #4 marks our best percentile achievement: Top 20.8%!** The combination of healthcare domain knowledge, external dataset leverage, and advanced ensemble techniques proved synergistic. We didn't just predict diabetes probabilitiesâ€”we engineered medical wisdom into machine learning. As dataset creators, seeing our Diabetes Health Indicators Dataset guide the very competition we competed in demonstrates the power of domain expertise. The climb continues toward Expert and Master status. ğŸš€

---

## ğŸ™ **ACKNOWLEDGMENTS**

- **Kaggle** for the incredible Playground Series
- **Yao Yan, Walter Reade, Elizabeth Park** for organizing S5E12
- **Our Team** (Mohan, Rakesh, Ranjith, Uday) for creating the original Diabetes Health Indicators Dataset
- **International Diabetes Federation (IDF)** - Dataset inspiration
- **Centers for Disease Control (CDC)** - Clinical guidelines reference
- **World Health Organization (WHO)** - Medical standards
- **Open Source:** XGBoost, LightGBM, CatBoost, scikit-learn teams
- **Our Competitor Community** - Pushing us to excellence

---

<div align="center">

# ğŸ¥ HEALTHCARE PREDICTIONS MASTERED ğŸ¥

## *"Competition #1: Top 48% | Competition #2: Top 23% | Competition #3: Top 33% | Competition #4: Top 20.8% ğŸ¯"*

### ğŸ† TEAM PHOENIX ALGORITHMS ğŸ†

**From Learning â†’ Optimizing â†’ Mastering â†’ DOMINATING Healthcare AI**

---

### ğŸ“Š 20.8% Rank | ğŸ¥ Healthcare Mastery | ğŸš€ Momentum Unstoppable

---

## **PROGRESS TRAJECTORY**

```
ğŸ”¥ S5E9:  Top 48.3% â†’ Foundation (Beats-per-Minute)
ğŸš€ S5E10: Top 23.5% â†’ Breakthrough (Road Accident Risk)
âš¡ S5E11: Top 33.7% â†’ AutoML Discovery (Loan Payback)
ğŸ’Š S5E12: Top 20.8% â†’ Healthcare Excellence (Diabetes) â† BEST PERCENTILE!
```

---

*The phoenix soars higher with every flight. Expert status closing in. Grandmaster within sight.* â­

**[Competition Link](https://www.kaggle.com/competitions/playground-series-s5e12)** | **December 2025 - January 2026** | **#TeamPhoenixAlgorithms**

### ğŸ–ï¸ **On The Path to Expert & Master Status** ğŸ–ï¸

---

**Repository:** [Diabetes-Prediction-Challenge](https://github.com/mohan13krishna/Diabetes-Prediction-Challenge)  
**Best Private Score:** 0.69760  
**Best Public Score:** 0.70042  
**Final Rank:** 877 / 4206 (Top 20.8%)  
**Dataset Creator:** Mohan Krishna Thalla (Silver Medal ğŸ¥ˆ)  
**Status:** âœ… Competition Completed (January 2026) | ğŸ† Best Percentile Achievement

</div>
