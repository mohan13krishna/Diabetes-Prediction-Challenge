{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37ac8cf2",
   "metadata": {},
   "source": [
    "# V17: Feature Reduction Optimization\n",
    "\n",
    "An iterative refinement of the ensemble approach with optimized feature engineering and strategic model configuration adjustments.\n",
    "\n",
    "**Key Features:**\n",
    "- 75 base features (24 original + 3 medical)\n",
    "- 48 external features from Diabetes dataset\n",
    "- Memory optimization for large dataset handling\n",
    "- Adjusted hyperparameters (3000 estimators CV, 1000 final)\n",
    "- Feature selection reducing to 38 features\n",
    "- Isotonic calibration for probability refinement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a0634b",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7076989f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "\n",
    "print(\"V17\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ddd95d",
   "metadata": {},
   "source": [
    "## 2. Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39954db",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('/kaggle/input/playground-series-s5e12/train.csv')\n",
    "test  = pd.read_csv('/kaggle/input/playground-series-s5e12/test.csv')\n",
    "sub   = pd.read_csv('/kaggle/input/playground-series-s5e12/sample_submission.csv')\n",
    "orig  = pd.read_csv('/kaggle/input/diabetes-health-indicators-dataset/diabetes_dataset.csv')\n",
    "\n",
    "TARGET = 'diagnosed_diabetes'\n",
    "BASE = [col for col in train.columns if col not in ['id', TARGET]]\n",
    "CATS = train.select_dtypes('object').columns.tolist()\n",
    "NUMS = [col for col in BASE if col not in CATS]\n",
    "\n",
    "print(f'{len(BASE)} Base Features.')\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd41acc",
   "metadata": {},
   "source": [
    "## 3. External Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97cc0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ORIG = []\n",
    "for col in BASE:\n",
    "    mean_map = orig.groupby(col)[TARGET].mean()\n",
    "    new_mean = f\"orig_mean_{col}\"\n",
    "    train[new_mean] = train[col].map(mean_map).fillna(orig[TARGET].mean())\n",
    "    test[new_mean] = test[col].map(mean_map).fillna(orig[TARGET].mean())\n",
    "    ORIG.append(new_mean)\n",
    "    \n",
    "    count_map = orig.groupby(col).size()\n",
    "    new_count = f\"orig_count_{col}\"\n",
    "    train[new_count] = train[col].map(count_map).fillna(0)\n",
    "    test[new_count] = test[col].map(count_map).fillna(0)\n",
    "    ORIG.append(new_count)\n",
    "\n",
    "print(f'{len(ORIG)} External Features.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bff134",
   "metadata": {},
   "source": [
    "## 4. Stable Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e553e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['bmi_cat'] = pd.cut(train['bmi'], bins=[0, 18.5, 25, 30, 100], labels=[0,1,2,3])\n",
    "test['bmi_cat'] = pd.cut(test['bmi'], bins=[0, 18.5, 25, 30, 100], labels=[0,1,2,3])\n",
    "\n",
    "train['bp_cat'] = 0\n",
    "train.loc[(train['systolic_bp'] >= 140) | (train['diastolic_bp'] >= 90), 'bp_cat'] = 2\n",
    "train.loc[((train['systolic_bp'] >= 120) & (train['systolic_bp'] < 140)) | ((train['diastolic_bp'] >= 80) & (train['diastolic_bp'] < 90)), 'bp_cat'] = 1\n",
    "test['bp_cat'] = 0\n",
    "test.loc[(test['systolic_bp'] >= 140) | (test['diastolic_bp'] >= 90), 'bp_cat'] = 2\n",
    "test.loc[((test['systolic_bp'] >= 120) & (test['systolic_bp'] < 140)) | ((test['diastolic_bp'] >= 80) & (test['diastolic_bp'] < 90)), 'bp_cat'] = 1\n",
    "\n",
    "train['non_hdl'] = train['cholesterol_total'] - train['hdl_cholesterol']\n",
    "test['non_hdl'] = test['cholesterol_total'] - test['hdl_cholesterol']\n",
    "\n",
    "NEW_FEATS = ['bmi_cat', 'bp_cat', 'non_hdl']\n",
    "for feat in NEW_FEATS:\n",
    "    BASE.append(feat)\n",
    "\n",
    "print(f'{len(NEW_FEATS)} Stable FE Features.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd620be",
   "metadata": {},
   "source": [
    "## 5. Memory Optimization\n",
    "\n",
    "Reduces memory footprint by downcasting numeric dtypes while preserving precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f6d817",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if col_type != object and col_type.name != 'category':\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    return df\n",
    "\n",
    "train = reduce_mem_usage(train)\n",
    "test = reduce_mem_usage(test)\n",
    "gc.collect()\n",
    "\n",
    "print(\"Memory optimization completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4ff353",
   "metadata": {},
   "source": [
    "## 6. Final Features & Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcacd38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = BASE + ORIG\n",
    "print(f'{len(FEATURES)} Total Features.')\n",
    "\n",
    "X = train[FEATURES]\n",
    "y = train[TARGET]\n",
    "\n",
    "# Safe Label Encoding\n",
    "ALL_CATS = CATS + ['bmi_cat', 'bp_cat']\n",
    "for col in ALL_CATS:\n",
    "    if col in X.columns:\n",
    "        le = LabelEncoder()\n",
    "        combined = pd.concat([X[col].astype(str), test[col].astype(str)])\n",
    "        le.fit(combined)\n",
    "        X[col] = le.transform(X[col].astype(str))\n",
    "        test[col] = le.transform(test[col].astype(str))\n",
    "\n",
    "X_test = test[FEATURES]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4952627",
   "metadata": {},
   "source": [
    "## 7. 10-Fold Ensemble with Ultra-Heavy Regularization\n",
    "\n",
    "3-model ensemble with adjusted hyperparameters:\n",
    "- 3000 estimators during CV (reduced from 5000 for faster iteration)\n",
    "- Ultra-heavy regularization (L1=3.0, L2=3.0-3.5)\n",
    "- Blend weights: 40% XGB, 35% LGBM, 25% CB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cee6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "oof = np.zeros(len(X))\n",
    "pred_xgb = np.zeros(len(X_test))\n",
    "pred_lgb = np.zeros(len(X_test))\n",
    "pred_cb = np.zeros(len(X_test))\n",
    "\n",
    "print(\"\\nTraining 10-Fold Ensemble with Ultra-Heavy Reg...\\n\")\n",
    "\n",
    "for fold, (trn_idx, val_idx) in enumerate(skf.split(X, y), 1):\n",
    "    print(f\"Fold {fold}/10 â†’ \", end=\"\")\n",
    "    \n",
    "    X_trn, X_val = X.iloc[trn_idx], X.iloc[val_idx]\n",
    "    y_trn, y_val = y.iloc[trn_idx], y.iloc[val_idx]\n",
    "    \n",
    "    # XGBoost (Ultra-Heavy Reg)\n",
    "    m1 = xgb.XGBClassifier(n_estimators=3000, max_depth=4, learning_rate=0.008,\n",
    "                           subsample=0.7, colsample_bytree=0.6, reg_alpha=3.0, reg_lambda=3.0,\n",
    "                           random_state=42, tree_method=\"hist\", n_jobs=-1, verbosity=0)\n",
    "    m1.fit(X_trn, y_trn, eval_set=[(X_val, y_val)], early_stopping_rounds=200, verbose=False)\n",
    "    \n",
    "    # LightGBM (Ultra-Heavy Reg)\n",
    "    m2 = lgb.LGBMClassifier(n_estimators=3000, max_depth=4, learning_rate=0.008,\n",
    "                            num_leaves=20, subsample=0.7, colsample_bytree=0.6,\n",
    "                            reg_alpha=3.0, reg_lambda=3.0, random_state=42, n_jobs=-1, verbose=-1)\n",
    "    m2.fit(X_trn, y_trn, eval_set=[(X_val, y_val)], callbacks=[lgb.early_stopping(200)])\n",
    "    \n",
    "    # CatBoost (Ultra-Heavy Reg)\n",
    "    m3 = cb.CatBoostClassifier(iterations=3000, depth=4, learning_rate=0.008,\n",
    "                               l2_leaf_reg=8.0, random_seed=42, verbose=False, early_stopping_rounds=200)\n",
    "    m3.fit(X_trn, y_trn, eval_set=(X_val, y_val))\n",
    "    \n",
    "    # Blend\n",
    "    val_pred = (m1.predict_proba(X_val)[:,1] * 0.4 + m2.predict_proba(X_val)[:,1] * 0.35 + m3.predict_proba(X_val)[:,1] * 0.25)\n",
    "    oof[val_idx] = val_pred\n",
    "    \n",
    "    pred_xgb += m1.predict_proba(X_test)[:,1] / skf.n_splits\n",
    "    pred_lgb += m2.predict_proba(X_test)[:,1] / skf.n_splits\n",
    "    pred_cb  += m3.predict_proba(X_test)[:,1] / skf.n_splits\n",
    "    \n",
    "    fold_auc = roc_auc_score(y_val, val_pred)\n",
    "    print(f\"AUC = {fold_auc:.6f}\")\n",
    "\n",
    "print(f\"\\nFinal CV AUC: {roc_auc_score(y, oof):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6989939b",
   "metadata": {},
   "source": [
    "## 8. Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7f1997",
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = SelectFromModel(m1, threshold='median', prefit=True)\n",
    "X_selected = selector.transform(X)\n",
    "X_test_selected = selector.transform(X_test)\n",
    "\n",
    "print(f\"Selected {X_selected.shape[1]} features out of {X.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1e39c9",
   "metadata": {},
   "source": [
    "## 9. Quick Re-train on Selected Features (XGB only for speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2088df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_final = xgb.XGBClassifier(n_estimators=1000, max_depth=4, learning_rate=0.01,\n",
    "                            subsample=0.7, colsample_bytree=0.6, reg_alpha=2.0, reg_lambda=2.0,\n",
    "                            random_state=42, tree_method=\"hist\", n_jobs=-1, verbosity=0)\n",
    "m_final.fit(X_selected, y)\n",
    "\n",
    "final_pred = m_final.predict_proba(X_test_selected)[:,1]\n",
    "print(f\"Final model trained on {X_selected.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302d3493",
   "metadata": {},
   "source": [
    "## 10. Isotonic Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6c7ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "calib = IsotonicRegression(out_of_bounds='clip')\n",
    "calib.fit(oof, y)\n",
    "final_pred = calib.transform(final_pred)\n",
    "print(\"Isotonic calibration applied\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899f4bf3",
   "metadata": {},
   "source": [
    "## 11. Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e26083",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub[TARGET] = final_pred\n",
    "sub.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"\\nsubmission.csv saved!\")\n",
    "print(f'Mean predicted: {final_pred.mean():.5f}')\n",
    "print(f'Min predicted: {final_pred.min():.5f}')\n",
    "print(f'Max predicted: {final_pred.max():.5f}')\n",
    "\n",
    "print(\"\\nFirst few predictions:\")\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c579fb",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**V17 Optimizations:**\n",
    "- 75 total features (27 base + 3 medical + 48 external)\n",
    "- 3000 estimators for faster CV iteration\n",
    "- Adjusted blend weights (40/35/25 instead of 50/35/15)\n",
    "- Feature selection to 38 features\n",
    "- 1000-estimator final model\n",
    "- Memory optimization for dataset handling\n",
    "\n",
    "V17 represents a middle-ground approach balancing computational efficiency with ensemble robustness."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
