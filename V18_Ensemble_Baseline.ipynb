{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "113a02b1",
   "metadata": {},
   "source": [
    "# V18: Ensemble Baseline Solution\n",
    "\n",
    "A foundational ensemble approach combining XGBoost, LightGBM, and CatBoost with ultra-heavy regularization. This version establishes the baseline ensemble architecture used across multiple variations.\n",
    "\n",
    "**Key Features:**\n",
    "- 3-model ensemble (XGB, LGBM, CB) with weighted averaging\n",
    "- External feature encoding from 100K Diabetes dataset\n",
    "- Medical domain features (BMI categories, BP categories, non-HDL cholesterol)\n",
    "- 10-Fold Stratified Cross-Validation\n",
    "- Isotonic calibration for probability refinement\n",
    "- Feature selection using SelectFromModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7586558b",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155aa4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "\n",
    "print(\"V18\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f480567",
   "metadata": {},
   "source": [
    "## 2. Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f3c1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('/kaggle/input/playground-series-s5e12/train.csv')\n",
    "test  = pd.read_csv('/kaggle/input/playground-series-s5e12/test.csv')\n",
    "sub   = pd.read_csv('/kaggle/input/playground-series-s5e12/sample_submission.csv')\n",
    "orig  = pd.read_csv('/kaggle/input/diabetes-health-indicators-dataset/diabetes_dataset.csv')\n",
    "\n",
    "TARGET = 'diagnosed_diabetes'\n",
    "\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")\n",
    "print(f\"External dataset shape: {orig.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6e8730",
   "metadata": {},
   "source": [
    "## 3. External Encoding\n",
    "\n",
    "Mean and count encodings from the 100K Diabetes Health Indicators Dataset provide external statistical relationships that improve model robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d361797a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_cols = [c for c in train.columns if c not in ['id', TARGET]]\n",
    "encoded = []\n",
    "\n",
    "for col in base_cols:\n",
    "    # Mean encoding\n",
    "    mean_map = orig.groupby(col)[TARGET].mean()\n",
    "    train[f\"enc_mean_{col}\"] = train[col].map(mean_map)\n",
    "    test[f\"enc_mean_{col}\"]  = test[col].map(mean_map)\n",
    "    encoded.append(f\"enc_mean_{col}\")\n",
    "    \n",
    "    # Count encoding (log-scaled)\n",
    "    count_map = orig.groupby(col).size()\n",
    "    train[f\"enc_cnt_{col}\"] = train[col].map(count_map).fillna(1)\n",
    "    test[f\"enc_cnt_{col}\"]  = test[col].map(count_map).fillna(1)\n",
    "    train[f\"enc_cnt_{col}\"] = np.log1p(train[f\"enc_cnt_{col}\"])\n",
    "    test[f\"enc_cnt_{col}\"]  = np.log1p(test[f\"enc_cnt_{col}\"])\n",
    "    encoded.append(f\"enc_cnt_{col}\")\n",
    "\n",
    "print(f\"Generated {len(encoded)} external features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7ddc4b",
   "metadata": {},
   "source": [
    "## 4. Stable Feature Engineering\n",
    "\n",
    "Medical domain features based on clinical standards:\n",
    "- **BMI Categories**: WHO classification (underweight, normal, overweight, obese)\n",
    "- **BP Categories**: AHA guidelines (normal, elevated, high)\n",
    "- **Non-HDL Cholesterol**: Total - HDL (cardiovascular risk indicator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd76d928",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['bmi_cat'] = pd.cut(train['bmi'], bins=[0,18.5,25,30,999], labels=[0,1,2,3]).astype('int')\n",
    "test['bmi_cat']  = pd.cut(test['bmi'],  bins=[0,18.5,25,30,999], labels=[0,1,2,3]).astype('int')\n",
    "\n",
    "train['bp_cat'] = 0\n",
    "train.loc[(train['systolic_bp']>=140)|(train['diastolic_bp']>=90), 'bp_cat'] = 2\n",
    "train.loc[((train['systolic_bp']>=120)&(train['systolic_bp']<140))|\n",
    "          ((train['diastolic_bp']>=80)&(train['diastolic_bp']<90)), 'bp_cat'] = 1\n",
    "\n",
    "test['bp_cat'] = 0\n",
    "test.loc[(test['systolic_bp']>=140)|(test['diastolic_bp']>=90), 'bp_cat'] = 2\n",
    "test.loc[((test['systolic_bp']>=120)&(test['systolic_bp']<140))|\n",
    "         ((test['diastolic_bp']>=80)&(test['diastolic_bp']<90)), 'bp_cat'] = 1\n",
    "\n",
    "train['non_hdl'] = train['cholesterol_total'] - train['hdl_cholesterol']\n",
    "test['non_hdl']  = test['cholesterol_total'] - test['hdl_cholesterol']\n",
    "\n",
    "print(\"Medical features created: BMI categories, BP categories, Non-HDL cholesterol\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22e9445",
   "metadata": {},
   "source": [
    "## 5. Final Feature Set Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc00ce20",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = base_cols + ['bmi_cat', 'bp_cat', 'non_hdl'] + encoded\n",
    "\n",
    "# Fill NaNs\n",
    "for f in encoded:\n",
    "    train[f] = train[f].fillna(train[f].median())\n",
    "    test[f]  = test[f].fillna(train[f].median())\n",
    "\n",
    "X      = train[features].copy()\n",
    "y      = train[TARGET]\n",
    "X_test = test[features].copy()\n",
    "\n",
    "# Label encode categoricals\n",
    "cat_cols = ['bmi_cat', 'bp_cat'] + train.select_dtypes('object').columns.tolist()\n",
    "for col in cat_cols:\n",
    "    if col in X.columns:\n",
    "        le = LabelEncoder()\n",
    "        X[col]      = le.fit_transform(X[col].astype(str))\n",
    "        X_test[col] = le.transform(X_test[col].astype(str))\n",
    "\n",
    "print(f\"Total features: {X.shape[1]}\")\n",
    "print(f\"Training set shape: {X.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ceaffe4",
   "metadata": {},
   "source": [
    "## 6. 10-Fold Ensemble + Out-of-Fold (OOF) Predictions\n",
    "\n",
    "Ultra-heavy regularization on all three models to prevent overfitting on 700K training samples:\n",
    "- **XGBoost**: L1=3.0, L2=3.5\n",
    "- **LightGBM**: L1=3.0, L2=3.5\n",
    "- **CatBoost**: L2=10.0\n",
    "\n",
    "Ensemble weights: 50% XGB + 35% LGBM + 15% CB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8414a17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 10\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "oof = np.zeros(len(X))\n",
    "test_preds = np.zeros(len(X_test))\n",
    "\n",
    "print(f\"\\nStarting {n_splits}-fold training...\\n\")\n",
    "\n",
    "for fold, (trn_idx, val_idx) in enumerate(skf.split(X, y), 1):\n",
    "    print(f\"Fold {fold}/{n_splits}\", end=\" → \")\n",
    "    \n",
    "    X_trn, X_val = X.iloc[trn_idx], X.iloc[val_idx]\n",
    "    y_trn, y_val = y.iloc[trn_idx], y.iloc[val_idx]\n",
    "\n",
    "    # XGBoost – Ultra Heavy Reg\n",
    "    model1 = xgb.XGBClassifier(\n",
    "        n_estimators=5000,\n",
    "        max_depth=4,\n",
    "        learning_rate=0.008,\n",
    "        subsample=0.7,\n",
    "        colsample_bytree=0.6,\n",
    "        reg_alpha=3.0,\n",
    "        reg_lambda=3.5,\n",
    "        random_state=42,\n",
    "        tree_method='hist',\n",
    "        n_jobs=-1,\n",
    "        verbosity=0\n",
    "    )\n",
    "    model1.fit(X_trn, y_trn, eval_set=[(X_val, y_val)], early_stopping_rounds=300, verbose=False)\n",
    "\n",
    "    # LightGBM – Ultra Heavy Reg\n",
    "    model2 = lgb.LGBMClassifier(\n",
    "        n_estimators=5000,\n",
    "        max_depth=4,\n",
    "        learning_rate=0.008,\n",
    "        num_leaves=20,\n",
    "        subsample=0.7,\n",
    "        colsample_bytree=0.6,\n",
    "        reg_alpha=3.0,\n",
    "        reg_lambda=3.5,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=-1\n",
    "    )\n",
    "    model2.fit(X_trn, y_trn, eval_set=[(X_val, y_val)], callbacks=[lgb.early_stopping(300)])\n",
    "\n",
    "    # CatBoost – Ultra Heavy Reg\n",
    "    model3 = cb.CatBoostClassifier(\n",
    "        iterations=5000,\n",
    "        depth=4,\n",
    "        learning_rate=0.008,\n",
    "        l2_leaf_reg=10.0,\n",
    "        random_seed=42,\n",
    "        verbose=False,\n",
    "        early_stopping_rounds=300\n",
    "    )\n",
    "    model3.fit(X_trn, y_trn, eval_set=(X_val, y_val), verbose=False)\n",
    "\n",
    "    # OOF\n",
    "    val_pred = (model1.predict_proba(X_val)[:,1] * 0.50 +\n",
    "                model2.predict_proba(X_val)[:,1] * 0.35 +\n",
    "                model3.predict_proba(X_val)[:,1] * 0.15)\n",
    "\n",
    "    oof[val_idx] = val_pred\n",
    "    print(f\"AUC = {roc_auc_score(y_val, val_pred):.6f}\")\n",
    "\n",
    "    # Test\n",
    "    test_preds += (model1.predict_proba(X_test)[:,1] * 0.50 +\n",
    "                   model2.predict_proba(X_test)[:,1] * 0.35 +\n",
    "                   model3.predict_proba(X_test)[:,1] * 0.15) / n_splits\n",
    "\n",
    "print(f\"\\nFinal CV AUC: {roc_auc_score(y, oof):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef85ac1",
   "metadata": {},
   "source": [
    "## 7. Feature Selection\n",
    "\n",
    "SelectFromModel reduces feature dimensionality using median importance threshold from the first fold model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8e030d",
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = SelectFromModel(model1, threshold='median', prefit=True)\n",
    "X_sel = selector.transform(X)\n",
    "X_test_sel = selector.transform(X_test)\n",
    "print(f\"Selected {X_sel.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78de72f7",
   "metadata": {},
   "source": [
    "## 8. Final Model on Selected Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198980be",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = xgb.XGBClassifier(\n",
    "    n_estimators=2000,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.01,\n",
    "    subsample=0.7,\n",
    "    colsample_bytree=0.6,\n",
    "    reg_alpha=2.5,\n",
    "    reg_lambda=3.0,\n",
    "    random_state=42,\n",
    "    tree_method='hist',\n",
    "    n_jobs=-1,\n",
    "    verbosity=0\n",
    ")\n",
    "final_model.fit(X_sel, y)\n",
    "\n",
    "final_pred = final_model.predict_proba(X_test_sel)[:,1]\n",
    "print(f\"Final predictions generated: {final_pred.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd0fd85",
   "metadata": {},
   "source": [
    "## 9. Isotonic Calibration\n",
    "\n",
    "Refines probability predictions using isotonic regression fitted on OOF predictions, ensuring better probability calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1c04d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibrator = IsotonicRegression(out_of_bounds='clip')\n",
    "calibrator.fit(oof, y)\n",
    "final_pred = calibrator.transform(final_pred)\n",
    "print(f\"Predictions calibrated successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53eeeef",
   "metadata": {},
   "source": [
    "## 10. Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ae4141",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub[TARGET] = final_pred\n",
    "sub.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"\\nsubmission.csv saved!\")\n",
    "print(f\"Mean prediction: {final_pred.mean():.5f}\")\n",
    "print(f\"Min prediction: {final_pred.min():.5f}\")\n",
    "print(f\"Max prediction: {final_pred.max():.5f}\")\n",
    "\n",
    "print(\"\\nFirst few predictions:\")\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03871cd",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**V18 Architecture:**\n",
    "- 75 base + 48 external + 3 medical = 126 total features initially\n",
    "- 10-Fold CV with 3-model ensemble (XGB/LGBM/CB)\n",
    "- Ultra-heavy regularization prevents overfitting\n",
    "- Feature selection reduces to 38 features\n",
    "- Final model with 2000 estimators\n",
    "- Isotonic calibration for probability refinement\n",
    "\n",
    "This version establishes the baseline ensemble approach that subsequent versions refine and experiment with."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
