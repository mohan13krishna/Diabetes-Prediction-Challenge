{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ea801f8",
   "metadata": {},
   "source": [
    "# V15: Stacking with Meta-Learner\n",
    "\n",
    "Advanced ensemble approach using stacking with Logistic Regression as a meta-learner. Collects Out-of-Fold (OOF) predictions from three base models (XGB, LGBM, CB) and learns optimal ensemble weights.\n",
    "\n",
    "**Key Features:**\n",
    "- 3-model ensemble base: XGBoost, LightGBM, CatBoost\n",
    "- Out-of-Fold (OOF) collection for stacking\n",
    "- Logistic Regression meta-learner for optimal blending\n",
    "- External feature encoding from 100K dataset\n",
    "- Medical domain features (BMI, BP, non-HDL)\n",
    "- 10-Fold Stratified Cross-Validation\n",
    "- Probability clipping for calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce93ac47",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5ea8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "\n",
    "print(\"V15\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1967326a",
   "metadata": {},
   "source": [
    "## 2. Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef912fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('/kaggle/input/playground-series-s5e12/train.csv')\n",
    "test  = pd.read_csv('/kaggle/input/playground-series-s5e12/test.csv')\n",
    "sub   = pd.read_csv('/kaggle/input/playground-series-s5e12/sample_submission.csv')\n",
    "orig  = pd.read_csv('/kaggle/input/diabetes-health-indicators-dataset/diabetes_dataset.csv')\n",
    "\n",
    "TARGET = 'diagnosed_diabetes'\n",
    "\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")\n",
    "print(f\"External dataset shape: {orig.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40c65b1",
   "metadata": {},
   "source": [
    "## 3. External Encoding\n",
    "\n",
    "Mean and count encodings from the 100K diabetes health indicators dataset provide statistical relationships between features and target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e927d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_cols = [c for c in train.columns if c not in ['id', TARGET]]\n",
    "encoded = []\n",
    "\n",
    "for col in base_cols:\n",
    "    mean_map = orig.groupby(col)[TARGET].mean()\n",
    "    train[f\"enc_mean_{col}\"] = train[col].map(mean_map)\n",
    "    test[f\"enc_mean_{col}\"]  = test[col].map(mean_map)\n",
    "    encoded.append(f\"enc_mean_{col}\")\n",
    "    \n",
    "    count_map = orig.groupby(col).size()\n",
    "    train[f\"enc_cnt_{col}\"] = train[col].map(count_map).fillna(1)\n",
    "    test[f\"enc_cnt_{col}\"]  = test[col].map(count_map).fillna(1)\n",
    "    train[f\"enc_cnt_{col}\"] = np.log1p(train[f\"enc_cnt_{col}\"])\n",
    "    test[f\"enc_cnt_{col}\"]  = np.log1p(test[f\"enc_cnt_{col}\"])\n",
    "    encoded.append(f\"enc_cnt_{col}\")\n",
    "\n",
    "print(f\"Generated {len(encoded)} external features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfab630",
   "metadata": {},
   "source": [
    "## 4. Safe Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2aa54d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['bmi_cat'] = pd.cut(train['bmi'], bins=[0,18.5,25,30,999], labels=[0,1,2,3]).astype('int')\n",
    "test['bmi_cat']  = pd.cut(test['bmi'],  bins=[0,18.5,25,30,999], labels=[0,1,2,3]).astype('int')\n",
    "\n",
    "train['bp_cat'] = 0\n",
    "train.loc[(train['systolic_bp']>=140)|(train['diastolic_bp']>=90), 'bp_cat'] = 2\n",
    "train.loc[((train['systolic_bp']>=120)&(train['systolic_bp']<140))|\n",
    "          ((train['diastolic_bp']>=80)&(train['diastolic_bp']<90)), 'bp_cat'] = 1\n",
    "\n",
    "test['bp_cat'] = 0\n",
    "test.loc[(test['systolic_bp']>=140)|(test['diastolic_bp']>=90), 'bp_cat'] = 2\n",
    "test.loc[((test['systolic_bp']>=120)&(test['systolic_bp']<140))|\n",
    "         ((test['diastolic_bp']>=80)&(test['diastolic_bp']<90)), 'bp_cat'] = 1\n",
    "\n",
    "train['non_hdl'] = train['cholesterol_total'] - train['hdl_cholesterol']\n",
    "test['non_hdl']  = test['cholesterol_total'] - test['hdl_cholesterol']\n",
    "\n",
    "print(\"Medical features created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43dfb53",
   "metadata": {},
   "source": [
    "## 5. Final Features Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b439b422",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = base_cols + ['bmi_cat', 'bp_cat', 'non_hdl'] + encoded\n",
    "\n",
    "# Fill NaNs\n",
    "for f in encoded:\n",
    "    train[f] = train[f].fillna(train[f].median())\n",
    "    test[f]  = test[f].fillna(train[f].median())\n",
    "\n",
    "X      = train[features].copy()\n",
    "y      = train[TARGET]\n",
    "X_test = test[features].copy()\n",
    "\n",
    "# Label encode categoricals\n",
    "cat_cols = ['bmi_cat', 'bp_cat'] + train.select_dtypes('object').columns.tolist()\n",
    "for col in cat_cols:\n",
    "    if col in X.columns:\n",
    "        le = LabelEncoder()\n",
    "        X[col]      = le.fit_transform(X[col].astype(str))\n",
    "        X_test[col] = le.transform(X_test[col].astype(str))\n",
    "\n",
    "print(f\"Total features: {X.shape[1]}\")\n",
    "print(f\"Training set shape: {X.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000dc18f",
   "metadata": {},
   "source": [
    "## 6. 10-Fold Ensemble + OOF for Stacking\n",
    "\n",
    "Collect Out-of-Fold predictions from three diverse base models. OOF predictions will be used to train a meta-learner.\n",
    "- **XGBoost**: max_depth=6, learning_rate=0.01 (deeper trees)\n",
    "- **LightGBM**: max_depth=7, learning_rate=0.01 (deeper trees)\n",
    "- **CatBoost**: depth=7, learning_rate=0.01 (deeper trees)\n",
    "\n",
    "Ensemble weights: 50% XGB + 35% LGBM + 15% CB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459facea",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 10\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "oof_xgb = np.zeros(len(X))\n",
    "oof_lgb = np.zeros(len(X))\n",
    "oof_cb  = np.zeros(len(X))\n",
    "\n",
    "test_xgb = np.zeros(len(X_test))\n",
    "test_lgb = np.zeros(len(X_test))\n",
    "test_cb  = np.zeros(len(X_test))\n",
    "\n",
    "print(f\"\\nTraining {n_splits}-fold ensemble + collecting OOF...\\n\")\n",
    "\n",
    "for fold, (trn_idx, val_idx) in enumerate(skf.split(X, y), 1):\n",
    "    print(f\"Fold {fold}/{n_splits}\", end=\" â†’ \")\n",
    "    \n",
    "    X_trn, X_val = X.iloc[trn_idx], X.iloc[val_idx]\n",
    "    y_trn, y_val = y.iloc[trn_idx], y.iloc[val_idx]\n",
    "\n",
    "    # XGBoost\n",
    "    model1 = xgb.XGBClassifier(\n",
    "        n_estimators=5000, max_depth=6, learning_rate=0.01,\n",
    "        subsample=0.8, colsample_bytree=0.6,\n",
    "        reg_alpha=1.5, reg_lambda=2.0,\n",
    "        random_state=42, tree_method='hist', n_jobs=-1, verbosity=0\n",
    "    )\n",
    "    model1.fit(X_trn, y_trn, eval_set=[(X_val, y_val)], early_stopping_rounds=250, verbose=False)\n",
    "\n",
    "    # LightGBM\n",
    "    model2 = lgb.LGBMClassifier(\n",
    "        n_estimators=5000, max_depth=7, learning_rate=0.01,\n",
    "        num_leaves=48, subsample=0.8, colsample_bytree=0.6,\n",
    "        reg_alpha=1.5, reg_lambda=2.2, random_state=42, n_jobs=-1, verbose=-1\n",
    "    )\n",
    "    model2.fit(X_trn, y_trn, eval_set=[(X_val, y_val)], callbacks=[lgb.early_stopping(250)])\n",
    "\n",
    "    # CatBoost\n",
    "    model3 = cb.CatBoostClassifier(\n",
    "        iterations=5000, depth=7, learning_rate=0.01,\n",
    "        l2_leaf_reg=6.0, random_seed=42, verbose=False,\n",
    "        early_stopping_rounds=250\n",
    "    )\n",
    "    model3.fit(X_trn, y_trn, eval_set=(X_val, y_val), verbose=False)\n",
    "\n",
    "    # OOF predictions\n",
    "    oof_xgb[val_idx] = model1.predict_proba(X_val)[:,1]\n",
    "    oof_lgb[val_idx] = model2.predict_proba(X_val)[:,1]\n",
    "    oof_cb[val_idx]  = model3.predict_proba(X_val)[:,1]\n",
    "\n",
    "    # Test predictions\n",
    "    test_xgb += model1.predict_proba(X_test)[:,1] / n_splits\n",
    "    test_lgb += model2.predict_proba(X_test)[:,1] / n_splits\n",
    "    test_cb  += model3.predict_proba(X_test)[:,1] / n_splits\n",
    "\n",
    "    # Evaluate blend\n",
    "    val_blend = oof_xgb[val_idx]*0.50 + oof_lgb[val_idx]*0.35 + oof_cb[val_idx]*0.15\n",
    "    print(f\"AUC = {roc_auc_score(y_val, val_blend):.6f}\")\n",
    "\n",
    "print(f\"\\nFinal OOF CV AUC: {roc_auc_score(y, oof_xgb*0.50 + oof_lgb*0.35 + oof_cb*0.15):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e8c952",
   "metadata": {},
   "source": [
    "## 7. Stacking Meta-Learner\n",
    "\n",
    "Logistic Regression learns optimal ensemble weights by training on base model OOF predictions. This approach often outperforms manual weighted blending by discovering feature interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4438f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_train = np.column_stack([oof_xgb, oof_lgb, oof_cb])\n",
    "stack_test  = np.column_stack([test_xgb, test_lgb, test_cb])\n",
    "\n",
    "meta_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "meta_model.fit(stack_train, y)\n",
    "\n",
    "final_pred = meta_model.predict_proba(stack_test)[:,1]\n",
    "\n",
    "# Learned weights\n",
    "weights = np.exp(meta_model.coef_[0]) / np.sum(np.exp(meta_model.coef_[0]))\n",
    "print(f\"\\nLearned ensemble weights:\")\n",
    "print(f\"XGBoost: {weights[0]:.4f}\")\n",
    "print(f\"LightGBM: {weights[1]:.4f}\")\n",
    "print(f\"CatBoost: {weights[2]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9b483c",
   "metadata": {},
   "source": [
    "## 8. Final Calibration\n",
    "\n",
    "Clip probability predictions to [0.01, 0.99] for numerical stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c2f04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred = np.clip(final_pred, 0.01, 0.99)\n",
    "\n",
    "print(f\"Predictions clipped to [0.01, 0.99] range\")\n",
    "print(f\"Mean prediction: {final_pred.mean():.5f}\")\n",
    "print(f\"Min prediction: {final_pred.min():.5f}\")\n",
    "print(f\"Max prediction: {final_pred.max():.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaf0eb0",
   "metadata": {},
   "source": [
    "## 9. Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09663891",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub[TARGET] = final_pred\n",
    "sub.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"\\nsubmission.csv saved!\")\n",
    "print(f\"Mean prediction: {final_pred.mean():.5f}\")\n",
    "\n",
    "print(\"\\nFirst few predictions:\")\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8caef3",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**V15 Stacking Architecture:**\n",
    "- **Base Models**: XGB (d=6), LGBM (d=7), CatBoost (d=7) with 5000 estimators each\n",
    "- **Feature Set**: 75 features (24 base + 3 medical + 48 external)\n",
    "- **OOF Collection**: 10-fold stratified CV for stack training\n",
    "- **Meta-Learner**: Logistic Regression learns optimal base model weights\n",
    "- **Calibration**: Probability clipping to [0.01, 0.99]\n",
    "- **Expected Performance**: ~0.731 CV AUC\n",
    "\n",
    "V15 represents an advanced stacking approach that learns ensemble weights rather than using manual hyperparameter tuning. The Logistic Regression meta-learner can discover non-obvious feature interactions between base model predictions."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
