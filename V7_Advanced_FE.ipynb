{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faf7e0a1",
   "metadata": {},
   "source": [
    "# V7: Advanced Feature Engineering\n",
    "\n",
    "Optimized ensemble solution with advanced feature engineering including lifestyle risk scoring and interaction features. Builds on Optuna-tuned hyperparameters with a focus on capturing domain-specific signals.\n",
    "\n",
    "**Key Features:**\n",
    "- 77 total features (24 base + 5 engineered + 48 external)\n",
    "- Lifestyle risk scoring (diet, activity, smoking, alcohol)\n",
    "- Age × BMI interaction feature\n",
    "- Optuna-tuned hyperparameters (Trial 42)\n",
    "- 5-Fold Stratified Cross-Validation\n",
    "- 3-Model Ensemble (XGB + LGBM + CatBoost)\n",
    "- Weighted averaging (40/35/25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edaa2bc7",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd8cd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "\n",
    "print(\"V7: Advanced Feature Engineering Ensemble\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83a55c4",
   "metadata": {},
   "source": [
    "## 2. Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4109f7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('/kaggle/input/playground-series-s5e12/train.csv')\n",
    "test  = pd.read_csv('/kaggle/input/playground-series-s5e12/test.csv')\n",
    "sub   = pd.read_csv('/kaggle/input/playground-series-s5e12/sample_submission.csv')\n",
    "orig  = pd.read_csv('/kaggle/input/diabetes-health-indicators-dataset/diabetes_dataset.csv')\n",
    "\n",
    "print('Train Shape:', train.shape)\n",
    "print('Test Shape:', test.shape)\n",
    "print('Orig Shape:', orig.shape)\n",
    "\n",
    "TARGET = 'diagnosed_diabetes'\n",
    "BASE = [col for col in train.columns if col not in ['id', TARGET]]\n",
    "CATS = train.select_dtypes('object').columns.tolist()\n",
    "NUMS = [col for col in BASE if col not in CATS]\n",
    "\n",
    "print(f'{len(BASE)} Base Features.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bda27a6",
   "metadata": {},
   "source": [
    "## 3. External Features from Original Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16902a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ORIG = []\n",
    "for col in BASE:\n",
    "    mean_map = orig.groupby(col)[TARGET].mean()\n",
    "    new_mean = f\"orig_mean_{col}\"\n",
    "    train[new_mean] = train[col].map(mean_map).fillna(orig[TARGET].mean())\n",
    "    test[new_mean] = test[col].map(mean_map).fillna(orig[TARGET].mean())\n",
    "    ORIG.append(new_mean)\n",
    "    \n",
    "    count_map = orig.groupby(col).size()\n",
    "    new_count = f\"orig_count_{col}\"\n",
    "    train[new_count] = train[col].map(count_map).fillna(0)\n",
    "    test[new_count] = test[col].map(count_map).fillna(0)\n",
    "    ORIG.append(new_count)\n",
    "\n",
    "print(f'{len(ORIG)} External Features.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b53991",
   "metadata": {},
   "source": [
    "## 4. Advanced Feature Engineering\n",
    "\n",
    "Engineered features targeting diabetes risk factors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e9a036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BMI Categories (WHO Classification)\n",
    "train['bmi_cat'] = pd.cut(train['bmi'], bins=[0, 18.5, 25, 30, 100], labels=[0,1,2,3])\n",
    "test['bmi_cat'] = pd.cut(test['bmi'], bins=[0, 18.5, 25, 30, 100], labels=[0,1,2,3])\n",
    "\n",
    "# Blood Pressure Categories (AHA Guidelines)\n",
    "train['bp_cat'] = 0\n",
    "train.loc[(train['systolic_bp'] >= 140) | (train['diastolic_bp'] >= 90), 'bp_cat'] = 2\n",
    "train.loc[((train['systolic_bp'] >= 120) & (train['systolic_bp'] < 140)) | ((train['diastolic_bp'] >= 80) & (train['diastolic_bp'] < 90)), 'bp_cat'] = 1\n",
    "test['bp_cat'] = 0\n",
    "test.loc[(test['systolic_bp'] >= 140) | (test['diastolic_bp'] >= 90), 'bp_cat'] = 2\n",
    "test.loc[((test['systolic_bp'] >= 120) & (test['systolic_bp'] < 140)) | ((test['diastolic_bp'] >= 80) & (test['diastolic_bp'] < 90)), 'bp_cat'] = 1\n",
    "\n",
    "# Non-HDL Cholesterol (CVD Risk Marker)\n",
    "train['non_hdl'] = train['cholesterol_total'] - train['hdl_cholesterol']\n",
    "test['non_hdl'] = test['cholesterol_total'] - test['hdl_cholesterol']\n",
    "\n",
    "# Lifestyle Risk Score (Behavioral Factors)\n",
    "train['lifestyle_risk'] = (\n",
    "    (train['diet_score'] < 5).astype(int) +\n",
    "    (train['physical_activity_minutes_per_week'] < 150).astype(int) +\n",
    "    (train['smoking_status'] == 'Current').astype(int) +\n",
    "    (train['alcohol_consumption_per_week'] > 14).astype(int)\n",
    ")\n",
    "test['lifestyle_risk'] = (\n",
    "    (test['diet_score'] < 5).astype(int) +\n",
    "    (test['physical_activity_minutes_per_week'] < 150).astype(int) +\n",
    "    (test['smoking_status'] == 'Current').astype(int) +\n",
    "    (test['alcohol_consumption_per_week'] > 14).astype(int)\n",
    ")\n",
    "\n",
    "# Age × BMI Interaction (Metabolic Factor)\n",
    "train['age_bmi'] = train['age'] * train['bmi']\n",
    "test['age_bmi'] = test['age'] * test['bmi']\n",
    "\n",
    "NEW_FEATS = ['bmi_cat', 'bp_cat', 'non_hdl', 'lifestyle_risk', 'age_bmi']\n",
    "for feat in NEW_FEATS:\n",
    "    BASE.append(feat)\n",
    "\n",
    "print(f'{len(NEW_FEATS)} Advanced Features engineered.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648f1fd2",
   "metadata": {},
   "source": [
    "## 5. Memory Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4741f176",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if col_type != object and col_type.name != 'category':\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    return df\n",
    "\n",
    "train = reduce_mem_usage(train)\n",
    "test = reduce_mem_usage(test)\n",
    "gc.collect()\n",
    "\n",
    "print(\"Memory optimization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c14bef",
   "metadata": {},
   "source": [
    "## 6. Final Features & Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6447d24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = BASE + ORIG\n",
    "print(f'{len(FEATURES)} Total Features.')\n",
    "\n",
    "X = train[FEATURES]\n",
    "y = train[TARGET]\n",
    "\n",
    "# Safe Label Encoding for categorical variables\n",
    "ALL_CATS = CATS + ['bmi_cat', 'bp_cat']\n",
    "for col in ALL_CATS:\n",
    "    if col in X.columns:\n",
    "        le = LabelEncoder()\n",
    "        combined = pd.concat([X[col].astype(str), test[col].astype(str)])\n",
    "        le.fit(combined)\n",
    "        X[col] = le.transform(X[col].astype(str))\n",
    "        test[col] = le.transform(test[col].astype(str))\n",
    "\n",
    "X_test = test[FEATURES]\n",
    "print(f'X shape: {X.shape}')\n",
    "print(f'X_test shape: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7769e3dd",
   "metadata": {},
   "source": [
    "## 7. 5-Fold Ensemble with Optuna Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032c65f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "oof = np.zeros(len(X))\n",
    "pred_xgb = np.zeros(len(X_test))\n",
    "pred_lgb = np.zeros(len(X_test))\n",
    "pred_cb = np.zeros(len(X_test))\n",
    "\n",
    "# Optuna Trial 42 Best Parameters\n",
    "best_xgb_params = {\n",
    "    'n_estimators': 1342,\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.02535288408263534,\n",
    "    'subsample': 0.7904573035331046,\n",
    "    'colsample_bytree': 0.7693297580314381,\n",
    "    'reg_alpha': 0.9678790554111332,\n",
    "    'reg_lambda': 0.4496537845892851\n",
    "}\n",
    "\n",
    "print(\"\\nTraining 5-Fold Ensemble (Optuna Trial 42 + Advanced Features)...\\n\")\n",
    "\n",
    "for fold, (trn_idx, val_idx) in enumerate(skf.split(X, y), 1):\n",
    "    print(f\"Fold {fold}/5 → \", end=\"\")\n",
    "    \n",
    "    X_trn, X_val = X.iloc[trn_idx], X.iloc[val_idx]\n",
    "    y_trn, y_val = y.iloc[trn_idx], y.iloc[val_idx]\n",
    "    \n",
    "    # XGBoost with Optuna-tuned hyperparameters\n",
    "    m1 = xgb.XGBClassifier(**best_xgb_params, random_state=42, tree_method=\"hist\", \n",
    "                           n_jobs=-1, verbosity=0, enable_categorical=True)\n",
    "    m1.fit(X_trn, y_trn, eval_set=[(X_val, y_val)], early_stopping_rounds=100, verbose=False)\n",
    "    \n",
    "    # LightGBM with Optuna-adapted hyperparameters\n",
    "    m2 = lgb.LGBMClassifier(n_estimators=1342, max_depth=6, learning_rate=0.025,\n",
    "                            num_leaves=64, subsample=0.79, colsample_bytree=0.77,\n",
    "                            reg_alpha=0.97, reg_lambda=0.45, random_state=42, n_jobs=-1, verbose=-1)\n",
    "    m2.fit(X_trn, y_trn, eval_set=[(X_val, y_val)], callbacks=[lgb.early_stopping(100)])\n",
    "    \n",
    "    # CatBoost with Optuna-adapted hyperparameters\n",
    "    m3 = cb.CatBoostClassifier(iterations=1342, depth=6, learning_rate=0.025,\n",
    "                               l2_leaf_reg=0.45, random_seed=42, verbose=0,\n",
    "                               early_stopping_rounds=100)\n",
    "    m3.fit(X_trn, y_trn, eval_set=(X_val, y_val))\n",
    "    \n",
    "    # Weighted ensemble predictions\n",
    "    val_pred = (\n",
    "        m1.predict_proba(X_val)[:,1] * 0.40 +\n",
    "        m2.predict_proba(X_val)[:,1] * 0.35 +\n",
    "        m3.predict_proba(X_val)[:,1] * 0.25\n",
    "    )\n",
    "    oof[val_idx] = val_pred\n",
    "    \n",
    "    pred_xgb += m1.predict_proba(X_test)[:,1] * 0.40\n",
    "    pred_lgb += m2.predict_proba(X_test)[:,1] * 0.35 / skf.n_splits\n",
    "    pred_cb  += m3.predict_proba(X_test)[:,1] * 0.25 / skf.n_splits\n",
    "    \n",
    "    fold_auc = roc_auc_score(y_val, val_pred)\n",
    "    print(f\"AUC = {fold_auc:.6f}\")\n",
    "\n",
    "print(f\"\\nFinal CV AUC: {roc_auc_score(y, oof):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e142fa8",
   "metadata": {},
   "source": [
    "## 8. Generate Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d28be1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions = (pred_xgb + pred_lgb + pred_cb) / 1\n",
    "\n",
    "sub[TARGET] = final_predictions\n",
    "sub.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"\\nsubmission.csv saved!\")\n",
    "print(f'Mean predicted: {final_predictions.mean():.5f}')\n",
    "print(f'Min predicted: {final_predictions.min():.5f}')\n",
    "print(f'Max predicted: {final_predictions.max():.5f}')\n",
    "\n",
    "print(\"\\nFirst few predictions:\")\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdd4925",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**V7 Architecture:**\n",
    "- 77 total features (24 base + 5 engineered + 48 external)\n",
    "- **Advanced Feature Engineering:**\n",
    "  - Lifestyle Risk Score: 4-factor composite (diet, activity, smoking, alcohol)\n",
    "  - Age × BMI Interaction: metabolic syndrome risk marker\n",
    "  - WHO BMI Categories: medically-validated obesity risk\n",
    "  - AHA BP Categories: hypertension classification\n",
    "  - Non-HDL Cholesterol: cardiovascular risk indicator\n",
    "- Optuna Trial 42 Best Hyperparameters\n",
    "- 5-Fold Stratified Cross-Validation\n",
    "- 3-Model Ensemble: XGB (40%) + LGBM (35%) + CatBoost (25%)\n",
    "- Categorical support enabled\n",
    "- **Expected CV AUC: ~0.7304**\n",
    "\n",
    "V7 combines domain-knowledge feature engineering with data-driven Optuna hyperparameter optimization to capture both medical signals and statistical patterns in diabetes prediction."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
