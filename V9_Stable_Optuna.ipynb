{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5098409d",
   "metadata": {},
   "source": [
    "# V9: Stable Optuna-Tuned Ensemble\n",
    "\n",
    "Clean, stable ensemble using best Optuna-tuned hyperparameters (Trial 42) with efficient 5-fold cross-validation. This version focuses on reproducibility and robust generalization with minimal hyperparameter tuning overhead.\n",
    "\n",
    "**Key Features:**\n",
    "- 75 total features (24 base + 3 medical + 48 external)\n",
    "- Optuna Trial 42 best hyperparameters\n",
    "- 5-Fold Stratified Cross-Validation\n",
    "- 3-Model Ensemble (XGB + LGBM + CatBoost)\n",
    "- 1342 estimators per model (converged)\n",
    "- Weighted averaging (40/35/25)\n",
    "- Learning rate: 0.02535 (conservative)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1f5d37",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0eb49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "\n",
    "print(\"V9: Stable Optuna-Tuned Ensemble\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0911c491",
   "metadata": {},
   "source": [
    "## 2. Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6948f627",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('/kaggle/input/playground-series-s5e12/train.csv')\n",
    "test  = pd.read_csv('/kaggle/input/playground-series-s5e12/test.csv')\n",
    "sub   = pd.read_csv('/kaggle/input/playground-series-s5e12/sample_submission.csv')\n",
    "orig  = pd.read_csv('/kaggle/input/diabetes-health-indicators-dataset/diabetes_dataset.csv')\n",
    "\n",
    "print('Train Shape:', train.shape)\n",
    "print('Test Shape:', test.shape)\n",
    "print('Orig Shape:', orig.shape)\n",
    "\n",
    "TARGET = 'diagnosed_diabetes'\n",
    "BASE = [col for col in train.columns if col not in ['id', TARGET]]\n",
    "CATS = train.select_dtypes('object').columns.tolist()\n",
    "NUMS = [col for col in BASE if col not in CATS]\n",
    "\n",
    "print(f'{len(BASE)} Base Features.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fdb5fd",
   "metadata": {},
   "source": [
    "## 3. External Features from Original Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ec4ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ORIG = []\n",
    "for col in BASE:\n",
    "    # Mean encoding\n",
    "    mean_map = orig.groupby(col)[TARGET].mean()\n",
    "    new_mean = f\"orig_mean_{col}\"\n",
    "    train[new_mean] = train[col].map(mean_map).fillna(orig[TARGET].mean())\n",
    "    test[new_mean] = test[col].map(mean_map).fillna(orig[TARGET].mean())\n",
    "    ORIG.append(new_mean)\n",
    "    \n",
    "    # Count encoding with log1p smoothing\n",
    "    count_map = orig.groupby(col).size()\n",
    "    new_count = f\"orig_count_{col}\"\n",
    "    train[new_count] = np.log1p(train[col].map(count_map).fillna(0))\n",
    "    test[new_count] = np.log1p(test[col].map(count_map).fillna(0))\n",
    "    ORIG.append(new_count)\n",
    "\n",
    "print(f'{len(ORIG)} External Features.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10e875c",
   "metadata": {},
   "source": [
    "## 4. Medical Domain Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27977e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BMI Categories (WHO Classification)\n",
    "train['bmi_cat'] = pd.cut(train['bmi'], bins=[0, 18.5, 25, 30, 100], labels=[0,1,2,3])\n",
    "test['bmi_cat'] = pd.cut(test['bmi'], bins=[0, 18.5, 25, 30, 100], labels=[0,1,2,3])\n",
    "\n",
    "# Blood Pressure Categories (AHA Guidelines)\n",
    "train['bp_cat'] = 0\n",
    "train.loc[(train['systolic_bp'] >= 140) | (train['diastolic_bp'] >= 90), 'bp_cat'] = 2\n",
    "train.loc[((train['systolic_bp'] >= 120) & (train['systolic_bp'] < 140)) | ((train['diastolic_bp'] >= 80) & (train['diastolic_bp'] < 90)), 'bp_cat'] = 1\n",
    "test['bp_cat'] = 0\n",
    "test.loc[(test['systolic_bp'] >= 140) | (test['diastolic_bp'] >= 90), 'bp_cat'] = 2\n",
    "test.loc[((test['systolic_bp'] >= 120) & (test['systolic_bp'] < 140)) | ((test['diastolic_bp'] >= 80) & (test['diastolic_bp'] < 90)), 'bp_cat'] = 1\n",
    "\n",
    "# Non-HDL Cholesterol (CVD Risk)\n",
    "train['non_hdl'] = train['cholesterol_total'] - train['hdl_cholesterol']\n",
    "test['non_hdl'] = test['cholesterol_total'] - test['hdl_cholesterol']\n",
    "\n",
    "print('Medical features engineered.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1adbb5",
   "metadata": {},
   "source": [
    "## 5. Memory Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078ef1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if col_type != object and col_type.name != 'category':\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    return df\n",
    "\n",
    "train = reduce_mem_usage(train)\n",
    "test = reduce_mem_usage(test)\n",
    "gc.collect()\n",
    "\n",
    "print(\"Memory optimization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af426b5",
   "metadata": {},
   "source": [
    "## 6. Feature Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe120838",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = BASE + ['bmi_cat', 'bp_cat', 'non_hdl'] + ORIG\n",
    "print(f'{len(FEATURES)} Total Features.')\n",
    "\n",
    "X = train[FEATURES]\n",
    "y = train[TARGET]\n",
    "\n",
    "# Safe Label Encoding\n",
    "ALL_CATS = CATS + ['bmi_cat', 'bp_cat']\n",
    "for col in ALL_CATS:\n",
    "    if col in X.columns:\n",
    "        le = LabelEncoder()\n",
    "        combined = pd.concat([X[col].astype(str), test[col].astype(str)])\n",
    "        le.fit(combined)\n",
    "        X[col] = le.transform(X[col].astype(str))\n",
    "        test[col] = le.transform(test[col].astype(str))\n",
    "\n",
    "X_test = test[FEATURES]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a37094",
   "metadata": {},
   "source": [
    "## 7. 5-Fold Cross-Validation with Optuna Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab46353",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "oof = np.zeros(len(X))\n",
    "pred_test = np.zeros(len(X_test))\n",
    "\n",
    "# Optuna Trial 42 Best Hyperparameters\n",
    "best_xgb_params = {\n",
    "    'n_estimators': 1342,\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.02535288408263534,\n",
    "    'subsample': 0.7904573035331046,\n",
    "    'colsample_bytree': 0.7693297580314381,\n",
    "    'reg_alpha': 0.9678790554111332,\n",
    "    'reg_lambda': 0.4496537845892851\n",
    "}\n",
    "\n",
    "print(\"\\nTraining 5-Fold Ensemble (Optuna Trial 42)...\\n\")\n",
    "\n",
    "for fold, (trn_idx, val_idx) in enumerate(skf.split(X, y), 1):\n",
    "    print(f\"Fold {fold}/5 â†’ \", end=\"\")\n",
    "    \n",
    "    X_trn, X_val = X.iloc[trn_idx], X.iloc[val_idx]\n",
    "    y_trn, y_val = y.iloc[trn_idx], y.iloc[val_idx]\n",
    "    \n",
    "    # XGBoost - Exact Optuna Trial 42 parameters\n",
    "    m1 = xgb.XGBClassifier(**best_xgb_params, random_state=42, tree_method=\"hist\",\n",
    "                           n_jobs=-1, verbosity=0)\n",
    "    m1.fit(X_trn, y_trn, eval_set=[(X_val, y_val)], early_stopping_rounds=100, verbose=False)\n",
    "    \n",
    "    # LightGBM - Adapted Optuna parameters\n",
    "    m2 = lgb.LGBMClassifier(n_estimators=1342, max_depth=6, learning_rate=0.025287,\n",
    "                            num_leaves=64, subsample=0.7905, colsample_bytree=0.7693,\n",
    "                            reg_alpha=0.9679, reg_lambda=0.4497, random_state=42,\n",
    "                            n_jobs=-1, verbose=-1)\n",
    "    m2.fit(X_trn, y_trn, eval_set=[(X_val, y_val)], callbacks=[lgb.early_stopping(100)])\n",
    "    \n",
    "    # CatBoost - Adapted Optuna parameters\n",
    "    m3 = cb.CatBoostClassifier(iterations=1342, depth=6, learning_rate=0.025287,\n",
    "                               l2_leaf_reg=0.4497, random_seed=42, verbose=0,\n",
    "                               early_stopping_rounds=100)\n",
    "    m3.fit(X_trn, y_trn, eval_set=(X_val, y_val))\n",
    "    \n",
    "    # Weighted ensemble\n",
    "    val_pred = (\n",
    "        m1.predict_proba(X_val)[:,1] * 0.40 +\n",
    "        m2.predict_proba(X_val)[:,1] * 0.35 +\n",
    "        m3.predict_proba(X_val)[:,1] * 0.25\n",
    "    )\n",
    "    oof[val_idx] = val_pred\n",
    "    \n",
    "    pred_test += (\n",
    "        m1.predict_proba(X_test)[:,1] * 0.40 +\n",
    "        m2.predict_proba(X_test)[:,1] * 0.35 +\n",
    "        m3.predict_proba(X_test)[:,1] * 0.25\n",
    "    ) / skf.n_splits\n",
    "    \n",
    "    fold_auc = roc_auc_score(y_val, val_pred)\n",
    "    print(f\"AUC = {fold_auc:.6f}\")\n",
    "\n",
    "cv_auc = roc_auc_score(y, oof)\n",
    "print(f\"\\nFinal CV AUC: {cv_auc:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fdebda",
   "metadata": {},
   "source": [
    "## 8. Generate Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d015eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub[TARGET] = pred_test\n",
    "sub.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"\\nsubmission.csv saved!\")\n",
    "print(f'Mean predicted: {pred_test.mean():.5f}')\n",
    "print(f'Min predicted: {pred_test.min():.5f}')\n",
    "print(f'Max predicted: {pred_test.max():.5f}')\n",
    "print(f'Std predicted: {pred_test.std():.5f}')\n",
    "\n",
    "print(\"\\nFirst few predictions:\")\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d00943",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**V9: Stable Optuna-Tuned Ensemble**\n",
    "\n",
    "**Architecture:**\n",
    "- 75 total features (24 base + 3 medical + 48 external)\n",
    "- **Hyperparameters**: Optuna Trial 42 best parameters\n",
    "  - Learning rate: 0.02535 (conservative, stable)\n",
    "  - n_estimators: 1342 (converged)\n",
    "  - max_depth: 6\n",
    "  - Regularization: reg_alpha=0.968, reg_lambda=0.450\n",
    "  - Subsample: 0.790, Colsample_bytree: 0.769\n",
    "- **Validation**: 5-Fold Stratified Cross-Validation\n",
    "- **Ensemble**: 3 models with proven weights\n",
    "  - XGBoost: 40% (SMAPE leader)\n",
    "  - LightGBM: 35% (Fast convergence)\n",
    "  - CatBoost: 25% (Regularization robustness)\n",
    "- **Expected CV AUC**: ~0.7305\n",
    "\n",
    "V9 prioritizes stability and reproducibility by using proven Optuna-tuned hyperparameters across a well-balanced 3-model ensemble."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
