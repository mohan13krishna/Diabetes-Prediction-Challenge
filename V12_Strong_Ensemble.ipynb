{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "523d06a7",
   "metadata": {},
   "source": [
    "# V12: Strong Ensemble with Early Stopping\n",
    "\n",
    "A robust ensemble approach combining three gradient boosting models with heavy regularization and strategic early stopping to prevent overfitting on the 700K training samples.\n",
    "\n",
    "**Key Features:**\n",
    "- 3-model ensemble: XGBoost, LightGBM, CatBoost\n",
    "- External feature encoding from 100K diabetes dataset\n",
    "- Medical domain features (BMI, BP, non-HDL)\n",
    "- 10-Fold Stratified Cross-Validation\n",
    "- Heavy regularization (L1=1.5, L2=2.0-2.2)\n",
    "- Weighted ensemble blending (50/35/15)\n",
    "- Probability clipping for calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4b7cae",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b57709",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "\n",
    "print(\"V12\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078b2b12",
   "metadata": {},
   "source": [
    "## 2. Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06205edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('/kaggle/input/playground-series-s5e12/train.csv')\n",
    "test  = pd.read_csv('/kaggle/input/playground-series-s5e12/test.csv')\n",
    "sub   = pd.read_csv('/kaggle/input/playground-series-s5e12/sample_submission.csv')\n",
    "orig  = pd.read_csv('/kaggle/input/diabetes-health-indicators-dataset/diabetes_dataset.csv')\n",
    "\n",
    "TARGET = 'diagnosed_diabetes'\n",
    "\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6337e5b9",
   "metadata": {},
   "source": [
    "## 3. External Encoding\n",
    "\n",
    "Mean and count (log-scaled) encodings from the 100K external diabetes dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dede27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_cols = [c for c in train.columns if c not in ['id', TARGET]]\n",
    "encoded = []\n",
    "\n",
    "for col in base_cols:\n",
    "    # Mean encoding\n",
    "    mapping = orig.groupby(col)[TARGET].mean()\n",
    "    train[f\"enc_mean_{col}\"] = train[col].map(mapping)\n",
    "    test[f\"enc_mean_{col}\"]  = test[col].map(mapping)\n",
    "    encoded.append(f\"enc_mean_{col}\")\n",
    "    \n",
    "    # Smoothed count encoding\n",
    "    cnt = orig.groupby(col).size()\n",
    "    train[f\"enc_cnt_{col}\"] = train[col].map(cnt).fillna(1)\n",
    "    test[f\"enc_cnt_{col}\"]  = test[col].map(cnt).fillna(1)\n",
    "    train[f\"enc_cnt_{col}\"] = np.log1p(train[f\"enc_cnt_{col}\"])\n",
    "    test[f\"enc_cnt_{col}\"]  = np.log1p(test[f\"enc_cnt_{col}\"])\n",
    "    encoded.append(f\"enc_cnt_{col}\")\n",
    "\n",
    "print(f\"Generated {len(encoded)} external features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4005be7",
   "metadata": {},
   "source": [
    "## 4. Safe & Strong Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062f4c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['bmi_cat'] = pd.cut(train['bmi'], bins=[0,18.5,25,30,999], labels=[0,1,2,3]).astype('int')\n",
    "test['bmi_cat']  = pd.cut(test['bmi'],  bins=[0,18.5,25,30,999], labels=[0,1,2,3]).astype('int')\n",
    "\n",
    "train['bp_cat'] = 0\n",
    "train.loc[(train['systolic_bp']>=140)|(train['diastolic_bp']>=90), 'bp_cat'] = 2\n",
    "train.loc[((train['systolic_bp']>=120)&(train['systolic_bp']<140))|\n",
    "          ((train['diastolic_bp']>=80)&(train['diastolic_bp']<90)), 'bp_cat'] = 1\n",
    "\n",
    "test['bp_cat'] = 0\n",
    "test.loc[(test['systolic_bp']>=140)|(test['diastolic_bp']>=90), 'bp_cat'] = 2\n",
    "test.loc[((test['systolic_bp']>=120)&(test['systolic_bp']<140))|\n",
    "         ((test['diastolic_bp']>=80)&(test['diastolic_bp']<90)), 'bp_cat'] = 1\n",
    "\n",
    "train['non_hdl'] = train['cholesterol_total'] - train['hdl_cholesterol']\n",
    "test['non_hdl']  = test['cholesterol_total'] - test['hdl_cholesterol']\n",
    "\n",
    "print(\"Medical features created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be2a286",
   "metadata": {},
   "source": [
    "## 5. Final Features + Label Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33caf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = base_cols + ['bmi_cat', 'bp_cat', 'non_hdl'] + encoded\n",
    "\n",
    "# Fill NaNs from encoding\n",
    "for f in encoded:\n",
    "    train[f] = train[f].fillna(train[f].median())\n",
    "    test[f]  = test[f].fillna(train[f].median())\n",
    "\n",
    "X      = train[features].copy()\n",
    "y      = train[TARGET]\n",
    "X_test = test[features].copy()\n",
    "\n",
    "# Label encode categoricals\n",
    "cat_cols = ['bmi_cat', 'bp_cat'] + train.select_dtypes('object').columns.tolist()\n",
    "for col in cat_cols:\n",
    "    if col in X.columns:\n",
    "        le = LabelEncoder()\n",
    "        X[col]      = le.fit_transform(X[col].astype(str))\n",
    "        X_test[col] = le.transform(X_test[col].astype(str))\n",
    "\n",
    "print(f\"Total features: {X.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb119a2",
   "metadata": {},
   "source": [
    "## 6. 10-Fold Strong Ensemble + Calibration\n",
    "\n",
    "Heavy regularization strategy:\n",
    "- **XGBoost**: max_depth=6, L1=1.5, L2=2.0, learning_rate=0.01\n",
    "- **LightGBM**: max_depth=7, L1=1.5, L2=2.2, learning_rate=0.01\n",
    "- **CatBoost**: depth=7, L2=6.0, learning_rate=0.01\n",
    "- **Ensemble weights**: 50% XGB + 35% LGBM + 15% CB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7b2755",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 10\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "oof = np.zeros(len(X))\n",
    "preds = np.zeros(len(X_test))\n",
    "\n",
    "print(f\"\\nStarting {n_splits}-fold training...\\n\")\n",
    "\n",
    "for fold, (trn_idx, val_idx) in enumerate(skf.split(X, y), 1):\n",
    "    print(f\"Fold {fold}/{n_splits}\", end=\" → \")\n",
    "    \n",
    "    X_trn, X_val = X.iloc[trn_idx], X.iloc[val_idx]\n",
    "    y_trn, y_val = y.iloc[trn_idx], y.iloc[val_idx]\n",
    "\n",
    "    # XGBoost – heavy regularization\n",
    "    model1 = xgb.XGBClassifier(\n",
    "        n_estimators=5000,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.01,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.6,\n",
    "        reg_alpha=1.5,\n",
    "        reg_lambda=2.0,\n",
    "        random_state=42,\n",
    "        tree_method='hist',\n",
    "        n_jobs=-1,\n",
    "        verbosity=0\n",
    "    )\n",
    "    model1.fit(X_trn, y_trn,\n",
    "               eval_set=[(X_val, y_val)],\n",
    "               early_stopping_rounds=250,\n",
    "               verbose=False)\n",
    "\n",
    "    # LightGBM – heavy regularization\n",
    "    model2 = lgb.LGBMClassifier(\n",
    "        n_estimators=5000,\n",
    "        max_depth=7,\n",
    "        learning_rate=0.01,\n",
    "        num_leaves=48,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.6,\n",
    "        reg_alpha=1.5,\n",
    "        reg_lambda=2.2,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=-1\n",
    "    )\n",
    "    model2.fit(X_trn, y_trn,\n",
    "               eval_set=[(X_val, y_val)],\n",
    "               callbacks=[lgb.early_stopping(250, verbose=False)])\n",
    "\n",
    "    # CatBoost\n",
    "    model3 = cb.CatBoostClassifier(\n",
    "        iterations=5000,\n",
    "        depth=7,\n",
    "        learning_rate=0.01,\n",
    "        l2_leaf_reg=6.0,\n",
    "        random_seed=42,\n",
    "        verbose=False,\n",
    "        early_stopping_rounds=250\n",
    "    )\n",
    "    model3.fit(X_trn, y_trn, eval_set=(X_val, y_val), verbose=False)\n",
    "\n",
    "    # Blend (best weights found by hill climbing)\n",
    "    val_pred = (model1.predict_proba(X_val)[:,1] * 0.50 +\n",
    "                model2.predict_proba(X_val)[:,1] * 0.35 +\n",
    "                model3.predict_proba(X_val)[:,1] * 0.15)\n",
    "\n",
    "    oof[val_idx] = val_pred\n",
    "    auc = roc_auc_score(y_val, val_pred)\n",
    "    print(f\"AUC = {auc:.6f}\")\n",
    "\n",
    "    # Test preds\n",
    "    preds += (model1.predict_proba(X_test)[:,1] * 0.50 +\n",
    "              model2.predict_proba(X_test)[:,1] * 0.35 +\n",
    "              model3.predict_proba(X_test)[:,1] * 0.15) / n_splits\n",
    "\n",
    "print(f\"\\nFinal CV AUC: {roc_auc_score(y, oof):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd495945",
   "metadata": {},
   "source": [
    "## 7. Final Submission\n",
    "\n",
    "Apply probability clipping to reduce overconfidence in extreme predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08e3bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred = preds\n",
    "final_pred = np.clip(final_pred, 0.01, 0.99)  # reduce overconfidence\n",
    "\n",
    "sub[TARGET] = final_pred\n",
    "sub.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"\\nsubmission.csv saved!\")\n",
    "print(f\"Mean prediction: {final_pred.mean():.5f}\")\n",
    "print(f\"Min prediction: {final_pred.min():.5f}\")\n",
    "print(f\"Max prediction: {final_pred.max():.5f}\")\n",
    "\n",
    "print(\"\\nFirst few predictions:\")\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacaeab5",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**V12 Architecture:**\n",
    "- 10-Fold Stratified Cross-Validation\n",
    "- 3-model ensemble with heavy regularization\n",
    "- 75 total features (24 base + 3 medical + 48 external)\n",
    "- Early stopping with 250 rounds patience\n",
    "- Weighted blending (50/35/15)\n",
    "- Probability clipping for calibration\n",
    "- Expected CV AUC: ~0.731\n",
    "\n",
    "V12 represents a strong intermediate approach balancing model diversity with rigorous regularization."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
