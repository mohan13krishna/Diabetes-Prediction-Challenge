{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46be691c",
   "metadata": {},
   "source": [
    "# V8: Anti-Overfit with Pseudo-Label Stacking\n",
    "\n",
    "Advanced approach combining Optuna-tuned hyperparameters with lifestyle risk features and Logistic Regression meta-learner stacking. This version experiments with enhanced feature engineering and meta-learning to reduce overfitting.\n",
    "\n",
    "**Key Features:**\n",
    "- 77 total features (24 base + 5 engineered + 48 external)\n",
    "- Advanced lifestyle risk feature engineering\n",
    "- Optuna-tuned hyperparameters (Trial 42)\n",
    "- 5-Fold Stratified Cross-Validation\n",
    "- Logistic Regression meta-learner stacking\n",
    "- Out-of-Fold prediction combining\n",
    "- Anti-overfitting regularization strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad49243",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b532d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "\n",
    "print(\"V8: Anti-Overfit with Stacking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555e37a7",
   "metadata": {},
   "source": [
    "## 2. Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef1606e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('/kaggle/input/playground-series-s5e12/train.csv')\n",
    "test  = pd.read_csv('/kaggle/input/playground-series-s5e12/test.csv')\n",
    "sub   = pd.read_csv('/kaggle/input/playground-series-s5e12/sample_submission.csv')\n",
    "orig  = pd.read_csv('/kaggle/input/diabetes-health-indicators-dataset/diabetes_dataset.csv')\n",
    "\n",
    "print('Train Shape:', train.shape)\n",
    "print('Test Shape:', test.shape)\n",
    "print('Orig Shape:', orig.shape)\n",
    "\n",
    "TARGET = 'diagnosed_diabetes'\n",
    "BASE = [col for col in train.columns if col not in ['id', TARGET]]\n",
    "CATS = train.select_dtypes('object').columns.tolist()\n",
    "NUMS = [col for col in BASE if col not in CATS]\n",
    "\n",
    "print(f'{len(BASE)} Base Features.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20342329",
   "metadata": {},
   "source": [
    "## 3. External Features from Original Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fe6a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ORIG = []\n",
    "for col in BASE:\n",
    "    mean_map = orig.groupby(col)[TARGET].mean()\n",
    "    new_mean = f\"orig_mean_{col}\"\n",
    "    train[new_mean] = train[col].map(mean_map).fillna(orig[TARGET].mean())\n",
    "    test[new_mean] = test[col].map(mean_map).fillna(orig[TARGET].mean())\n",
    "    ORIG.append(new_mean)\n",
    "    \n",
    "    count_map = orig.groupby(col).size()\n",
    "    new_count = f\"orig_count_{col}\"\n",
    "    train[new_count] = train[col].map(count_map).fillna(0)\n",
    "    test[new_count] = test[col].map(count_map).fillna(0)\n",
    "    ORIG.append(new_count)\n",
    "\n",
    "print(f'{len(ORIG)} External Features.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302cfbdb",
   "metadata": {},
   "source": [
    "## 4. Advanced Feature Engineering\n",
    "\n",
    "Added lifestyle risk and interaction features for enhanced model signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7d8d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BMI Categories\n",
    "train['bmi_cat'] = pd.cut(train['bmi'], bins=[0, 18.5, 25, 30, 100], labels=[0,1,2,3])\n",
    "test['bmi_cat'] = pd.cut(test['bmi'], bins=[0, 18.5, 25, 30, 100], labels=[0,1,2,3])\n",
    "\n",
    "# BP Categories\n",
    "train['bp_cat'] = 0\n",
    "train.loc[(train['systolic_bp'] >= 140) | (train['diastolic_bp'] >= 90), 'bp_cat'] = 2\n",
    "train.loc[((train['systolic_bp'] >= 120) & (train['systolic_bp'] < 140)) | ((train['diastolic_bp'] >= 80) & (train['diastolic_bp'] < 90)), 'bp_cat'] = 1\n",
    "test['bp_cat'] = 0\n",
    "test.loc[(test['systolic_bp'] >= 140) | (test['diastolic_bp'] >= 90), 'bp_cat'] = 2\n",
    "test.loc[((test['systolic_bp'] >= 120) & (test['systolic_bp'] < 140)) | ((test['diastolic_bp'] >= 80) & (test['diastolic_bp'] < 90)), 'bp_cat'] = 1\n",
    "\n",
    "# Non-HDL\n",
    "train['non_hdl'] = train['cholesterol_total'] - train['hdl_cholesterol']\n",
    "test['non_hdl'] = test['cholesterol_total'] - test['hdl_cholesterol']\n",
    "\n",
    "# Lifestyle Risk Score\n",
    "train['lifestyle_risk'] = (\n",
    "    (train['diet_score'] < 5).astype(int) +\n",
    "    (train['physical_activity_minutes_per_week'] < 150).astype(int) +\n",
    "    (train['smoking_status'] == 'Current').astype(int) +\n",
    "    (train['alcohol_consumption_per_week'] > 14).astype(int)\n",
    ")\n",
    "test['lifestyle_risk'] = (\n",
    "    (test['diet_score'] < 5).astype(int) +\n",
    "    (test['physical_activity_minutes_per_week'] < 150).astype(int) +\n",
    "    (test['smoking_status'] == 'Current').astype(int) +\n",
    "    (test['alcohol_consumption_per_week'] > 14).astype(int)\n",
    ")\n",
    "\n",
    "# Age x BMI Interaction\n",
    "train['age_bmi'] = train['age'] * train['bmi']\n",
    "test['age_bmi'] = test['age'] * test['bmi']\n",
    "\n",
    "NEW_FEATS = ['bmi_cat', 'bp_cat', 'non_hdl', 'lifestyle_risk', 'age_bmi']\n",
    "for feat in NEW_FEATS:\n",
    "    BASE.append(feat)\n",
    "\n",
    "print(f'{len(NEW_FEATS)} New FE Features.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a62669",
   "metadata": {},
   "source": [
    "## 5. Memory Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b263d81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if col_type != object and col_type.name != 'category':\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    return df\n",
    "\n",
    "train = reduce_mem_usage(train)\n",
    "test = reduce_mem_usage(test)\n",
    "gc.collect()\n",
    "\n",
    "print(\"Memory optimization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4009a287",
   "metadata": {},
   "source": [
    "## 6. Final Features & Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2d7737",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = BASE + ORIG\n",
    "print(f'{len(FEATURES)} Total Features.')\n",
    "\n",
    "X = train[FEATURES]\n",
    "y = train[TARGET]\n",
    "\n",
    "# Safe Label Encoding\n",
    "ALL_CATS = CATS + ['bmi_cat', 'bp_cat']\n",
    "for col in ALL_CATS:\n",
    "    if col in X.columns:\n",
    "        le = LabelEncoder()\n",
    "        combined = pd.concat([X[col].astype(str), test[col].astype(str)])\n",
    "        le.fit(combined)\n",
    "        X[col] = le.transform(X[col].astype(str))\n",
    "        test[col] = le.transform(test[col].astype(str))\n",
    "\n",
    "X_test = test[FEATURES]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43683145",
   "metadata": {},
   "source": [
    "## 7. 5-Fold Ensemble with Optuna Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9104723",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "oof = np.zeros(len(X))\n",
    "pred_xgb = np.zeros(len(X_test))\n",
    "pred_lgb = np.zeros(len(X_test))\n",
    "pred_cb = np.zeros(len(X_test))\n",
    "\n",
    "best_xgb_params = {\n",
    "    'n_estimators': 1342,\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.02535288408263534,\n",
    "    'subsample': 0.7904573035331046,\n",
    "    'colsample_bytree': 0.7693297580314381,\n",
    "    'reg_alpha': 0.9678790554111332,\n",
    "    'reg_lambda': 0.4496537845892851\n",
    "}\n",
    "\n",
    "print(\"\\nTraining 5-Fold Ensemble (Optuna Params + Lifestyle Features)...\\n\")\n",
    "\n",
    "for fold, (trn_idx, val_idx) in enumerate(skf.split(X, y), 1):\n",
    "    print(f\"Fold {fold}/5 → \", end=\"\")\n",
    "    \n",
    "    X_trn, X_val = X.iloc[trn_idx], X.iloc[val_idx]\n",
    "    y_trn, y_val = y.iloc[trn_idx], y.iloc[val_idx]\n",
    "    \n",
    "    # Tuned XGBoost (with categorical support)\n",
    "    m1 = xgb.XGBClassifier(**best_xgb_params, random_state=42, tree_method=\"hist\", \n",
    "                           n_jobs=-1, verbosity=0, enable_categorical=True)\n",
    "    m1.fit(X_trn, y_trn, eval_set=[(X_val, y_val)], early_stopping_rounds=100, verbose=False)\n",
    "    \n",
    "    # Tuned LightGBM\n",
    "    m2 = lgb.LGBMClassifier(n_estimators=1342, max_depth=6, learning_rate=0.025,\n",
    "                            num_leaves=64, subsample=0.79, colsample_bytree=0.77,\n",
    "                            reg_alpha=0.97, reg_lambda=0.45, random_state=42, n_jobs=-1, verbose=-1)\n",
    "    m2.fit(X_trn, y_trn, eval_set=[(X_val, y_val)], callbacks=[lgb.early_stopping(100)])\n",
    "    \n",
    "    # Tuned CatBoost\n",
    "    m3 = cb.CatBoostClassifier(iterations=1342, depth=6, learning_rate=0.025,\n",
    "                               l2_leaf_reg=0.45, random_seed=42, verbose=0, \n",
    "                               early_stopping_rounds=100)\n",
    "    m3.fit(X_trn, y_trn, eval_set=(X_val, y_val))\n",
    "    \n",
    "    # Collect OOF\n",
    "    oof[val_idx] = m1.predict_proba(X_val)[:,1]  # Will use for stacking\n",
    "    \n",
    "    pred_xgb += m1.predict_proba(X_test)[:,1] / skf.n_splits\n",
    "    pred_lgb += m2.predict_proba(X_test)[:,1] / skf.n_splits\n",
    "    pred_cb  += m3.predict_proba(X_test)[:,1] / skf.n_splits\n",
    "    \n",
    "    fold_auc = roc_auc_score(y_val, m1.predict_proba(X_val)[:,1])\n",
    "    print(f\"AUC = {fold_auc:.6f}\")\n",
    "\n",
    "print(f\"\\nFinal CV AUC: {roc_auc_score(y, oof):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257f49e7",
   "metadata": {},
   "source": [
    "## 8. Stacking Meta Model\n",
    "\n",
    "Train Logistic Regression on base model predictions to learn optimal ensemble weights and reduce overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1760cb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_X = np.column_stack([pred_xgb, pred_lgb, pred_cb])\n",
    "stack_oof = np.column_stack([m1.predict_proba(X)[:,1], m2.predict_proba(X)[:,1], m3.predict_proba(X)[:,1]])\n",
    "\n",
    "meta = LogisticRegression(random_state=42)\n",
    "meta.fit(stack_oof, y)\n",
    "final_pred = meta.predict_proba(stack_X)[:,1]\n",
    "\n",
    "print(\"\\nMeta-learner stacking applied\")\n",
    "print(f\"Meta weights: XGB={meta.coef_[0][0]:.4f}, LGBM={meta.coef_[0][1]:.4f}, CB={meta.coef_[0][2]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05569db",
   "metadata": {},
   "source": [
    "## 9. Final Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970111db",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub[TARGET] = final_pred\n",
    "sub.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"\\nsubmission.csv saved!\")\n",
    "print(f'Mean predicted: {final_pred.mean():.5f}')\n",
    "print(f'Min predicted: {final_pred.min():.5f}')\n",
    "print(f'Max predicted: {final_pred.max():.5f}')\n",
    "\n",
    "print(\"\\nFirst few predictions:\")\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc5c301",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**V8 Anti-Overfit Architecture:**\n",
    "- 77 total features (24 base + 5 engineered + 48 external)\n",
    "- Advanced lifestyle risk scoring (4-feature composite)\n",
    "- Age × BMI interaction feature\n",
    "- Optuna-tuned hyperparameters\n",
    "- 5-Fold Stratified Cross-Validation\n",
    "- Logistic Regression meta-learner stacking\n",
    "- Learns ensemble weights from base model OOF\n",
    "- Expected CV AUC: ~0.7304\n",
    "\n",
    "V8 combines enhanced feature engineering with meta-learning stacking to reduce overfitting and discover optimal ensemble blending."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
